# 高光谱图像分类研究进展：深度学习与机器学习的视角

# 1. 引言

## 1.1 高光谱图像处理的科学意义与应用需求

​	高光谱遥感技术通过其搭载的精密成像光谱仪，能够在目标地物区域获取近乎连续的、覆盖数百个窄光谱波段的图像数据。这种“图谱合一”的核心特性，使得高光谱图像（Hyperspectral Image, HSI）不仅能够精细刻画地物的空间分布，更能捕捉其细微的光谱反射特性，为地表物质的精准识别与定量分析提供了前所未有的高维信息支撑 [^1]。从信息学的角度看，每个像元所记录的光谱曲线犹如地物的“光谱指纹”，能够精确表征不同物质组分及其物理化学状态，这是传统多光谱或RGB遥感影像难以企及的 [^3]。

基于高光谱图像独特的价值，其在国家经济、社会发展和国家安全等关键领域展现出巨大的应用潜力。

- **在精准农业领域**，它为作物精细分类、长势监测、病虫害早期诊断及产量估算提供了关键技术手段，有力支持了现代农业的智能化管理与可持续发展 [^2]。
- **在生态环境监测方面**，它被广泛应用于水体富营养化评估、土壤属性（如有机质、重金属污染）定量检测、植被生理生化参数反演以及大气组分分析等，为生态保护和气候变化研究提供了重要科学依据 [^2]。
- **在城市精细化管理中**，高光谱数据能够精细区分不透水面、植被、水体等多种地表覆盖类型及其时空演变，服务于城市热岛效应分析、生态环境质量评价等应用场景 [^22]。
- **此外**，该技术在地质矿产勘查 [^9]、国防军事应用 [^9]、考古与文化遗产保护、食品安全无损检测 [^8] 及生物医学诊断 [^9] 等诸多前沿交叉领域，也正发挥着日益重要的、不可替代的作用。

高光谱图像处理，特别是其核心任务之一的图像分类，其技术水平与各领域的深化应用紧密相连。实际应用场景对分类结果的精度、鲁棒性、自动化及智能化处理效率提出了持续增长且日益严苛的要求，这成为驱动高光谱图像分类理论与技术不断创新的根本动力。



## 1.2 高光谱图像分类任务面临的核心挑战

​	尽管高光谱图像蕴含了极为丰富的地物信息，但其分类任务的有效实施面临着一系列源于数据自身特性及应用需求的固有挑战。这些挑战共同构成了高光谱图像分类问题的内在复杂性，并持续驱动着研究者们不懈探索更为有效的特征表征学习与智能分类建模方法。

​	首先，数据的高维特性与“休斯现象”（Hughes Phenomenon） 是高光谱图像分类面临的首要难题。高光谱图像通常包含数百个甚至数千个在光谱上连续分布的窄波段，导致数据特征维度极高。这种高维性一方面为地物的精细识别提供了丰富的信息基础，但另一方面也极易引发所谓的“维度灾难”或“休斯现象”：即在训练样本数量相对固定的情况下，当特征维度增加到一定阈值后，分类器的性能非但不能持续提升，反而会呈现显著下降的趋势[^4]。同时，高维数据也意味着巨大的信息冗余、复杂的噪声干扰以及显著增加的存储与计算复杂度，对算法的效率和可扩展性提出了严峻考验。

​	其次，有限的标记样本（Limited Labeled Samples） 是制约分类模型性能与泛化能力的另一关键瓶颈。获取高光谱图像的地面真实标签（Ground Truth）通常依赖于耗时、费力且成本高昂的野外实地调查、高精度地理定位、专业遥感影像目视解译以及实验室分析等手段。因此，在绝大多数实际应用中，可用于训练和验证分类模型的带标签样本往往非常稀缺，甚至在某些特定区域或特定地物类别上完全缺失[^4]。这种“小样本”困境使得依赖大量数据驱动的深度学习等复杂模型难以充分学习各类地物的完整特征分布规律，极易导致模型过拟合（Overfitting）于有限的训练数据，从而在新数据上的泛化能力严重不足。正如文献[^23]所深刻指出的，在同时面临高维高光谱数据和有限训练样本的条件下，如何实现高精度、高鲁棒性的地物分类仍然是一个亟待解决的开放性科学问题。

​	再次，复杂多变的光谱变异性（Spectral Variability） 对分类算法的适应性和鲁棒性构成了严峻挑战。高光谱图像中地物的光谱特征受到多种内外部复杂因素的综合影响，例如大气条件（水汽吸收、气溶胶散射）、光照几何条件（太阳高度角、方位角）、地形起伏（坡度、坡向）、传感器自身的系统噪声以及地物自身物理化学状态（如植被的含水量、叶绿素含量、物候期变化，土壤的湿度、有机质含量差异等）的动态变化。这些因素共同作用，导致同一种地物在不同时间、不同区域或不同观测条件下其光谱特征可能呈现显著差异，即所谓的“同物异谱”现象，表现为较大的类内变异性；与此同时，不同种类的地物由于其物质组成或表面结构上的某些相似性，其光谱特征又可能表现出高度的相似甚至混淆，即所谓的“异物同谱”现象，表现为较小的类间可分性，增加了类别判别的难度[^13]。文献[^18]明确指出，高的类间相似性、大的类内变异性以及不同类别光谱特征在特征空间中的重叠和嵌套，是导致高光谱图像分类任务极具挑战性的主要原因。

​	此外，混合像元（Mixed Pixels） 现象普遍存在于高光谱图像中，尤其是在中低空间分辨率的影像数据中。由于高光谱传感器的空间分辨率通常相对有限（例如，经典的机载AVIRIS传感器的典型空间分辨率为20米[^9]），地面上一个像元所覆盖的区域内往往包含多种不同类型的地物组分。这种混合像元的存在使得单个像元所记录的光谱特征实际上是其内部多种纯净地物（通常称为端元，Endmember）光谱的线性或非线性混合体，这不仅极大地增加了对地物进行精确分类和面积估算的难度[^1]，也对特征提取和分类模型的精细化程度提出了更高要求。

​	最后，噪声与异常值（Noise and Outliers） 也是影响高光谱图像数据质量和后续分类精度的不可忽视因素。高光谱数据在成像、信号传输、数据存储和预处理等各个环节，不可避免地会受到各种来源的噪声干扰（例如传感器暗电流噪声、光子散粒噪声、大气散射引入的周期性条带噪声等）以及异常值（例如由传感器故障或传输错误导致的坏线、坏点）的影响。这些干扰会显著降低光谱数据的信噪比，扭曲地物真实的光谱特征曲线形态，从而对后续的特征提取和分类精度产生不利影响[^16]。

​	综上所述，这些源于高光谱数据本身的“数据固有挑战”，特别是“小样本”学习困境与“高维特性”之间存在的尖锐矛盾，构成了驱动高光谱图像分类理论与方法不断演进的核心动力。这些挑战也催生了学术界对更高效、更鲁棒、更能充分利用高光谱数据中蕴含的空谱联合信息的先进智能模型（例如后续章节将重点综述的图神经网络、Transformer模型以及新兴的状态空间模型Mamba等）的持续探索与深入研究。从根本上认知并有效应对这些数据层面的挑战，是推动高光谱图像分类技术取得实质性突破的关键所在。

 

## 1.3 深度学习与机器学习在本领域的核心驱动作用

​	面对高光谱图像分类任务所呈现的诸多复杂挑战，以深度学习（Deep Learning, DL）为代表的人工智能技术和传统的机器学习（Machine Learning, ML）方法，尤其是前者，已成为当前该领域研究的主流方向和核心技术支撑。

​	传统的机器学习方法，例如支持向量机（Support Vector Machine, SVM）[^9]、随机森林（Random Forest, RF）[^9]、K近邻（K-Nearest Neighbors, KNN）以及逻辑回归（Logistic Regression）等[^4]，在高光谱图像分类的早期发展阶段扮演了重要角色，并在一定条件下取得了一定的成功。这些方法通常依赖于一个独立的人工特征提取或特征选择步骤（例如采用主成分分析PCA进行降维、利用线性判别分析LDA提取判别特征等），然后将经过处理的特征输入到设计好的分类器中进行类别判定。然而，这类手工设计的特征往往难以充分捕捉高光谱数据中复杂的高维非线性结构信息，并且在面对光谱变异性、混合像元等复杂问题时，其鲁棒性和自适应能力有限。当前，这些传统机器学习方法更多地被用作评估新型算法性能的基准模型（Baseline Models），或在数据量极小、计算资源严重受限的特定场景下得到应用[^27]。

​	近年来，深度学习方法凭借其强大的特征自动学习能力和卓越的非线性建模能力，在高光谱图像分类领域取得了突破性的进展，并迅速成为该领域的研究热点与前沿[^1]。深度学习模型，特别是卷积神经网络（Convolutional Neural Network, CNN）、循环神经网络（Recurrent Neural Network, RNN）及其重要变体如长短期记忆网络（Long Short-Term Memory, LSTM）和门控循环单元（Gated Recurrent Unit, GRU）、图神经网络（Graph Neural Network, GNN）、基于自注意力机制的Transformer架构以及新兴的状态空间模型（如Mamba），能够从原始高光谱数据中以一种端到端（End-to-End）的方式自动学习层次化的、具有高度判别性的深度特征表示，从而有效避免了传统机器学习方法中繁琐且依赖经验的特征工程环节。文献[^4]的一项系统性调查明确指出，基于深度学习的分类技术在总体性能上显著优于基于传统机器学习的技术，并且那些能够有效结合光谱与空间信息的深度学习高光谱图像分类方法，其性能通常优于仅依赖像元光谱信息的逐像素分类方法。文献[^1]亦强调，深度学习分类器，特别是以CNN为代表的模型，近年来对高光谱图像分类领域产生了深远且革命性的影响。文献[^8]则进一步指出，深度学习因其能够从图像数据中隐式地、自适应地提取与任务相关的抽象特征，已成为过去十年中高光谱图像分类的首选方法论。这些先进的深度学习模型及其灵活的混合架构，通过有效挖掘和利用高光谱数据中丰富的空谱联合信息，在应对高维性、光谱变异性、小样本学习等核心挑战方面展现出显著的优势和巨大的潜力。

​	值得注意的是，高光谱图像分类领域的研究也因此呈现出从早期的“单一模型创新”逐渐向“多模型融合与优势互补”演进的趋势。研究者们逐渐认识到，单一类型的深度学习模型（例如纯粹的CNN或纯粹的Transformer）往往各有其固有的优势和局限性。例如，CNN擅长提取具有平移不变性的局部空间上下文特征，Transformer则更擅长捕捉数据中的长距离全局依赖关系，而GNN则为处理像元间的复杂非欧式结构关系提供了独特的视角。通过巧妙地设计模型架构，将不同类型模型的优点进行有机结合与协同增强，往往能够获得比任何单一模型更优的分类性能。这标志着高光谱图像分类研究进入了一个更为成熟和精细化的阶段，研究者不再仅仅追求单一模型的极致性能，而是开始更加关注如何通过模块化的设计理念和新颖的融合策略，来更全面、更深入地理解和应对高光谱数据内在的复杂特性。

 

## 1.4 本文献综述的结构编排与主要内容

​	本文献综述旨在系统性地梳理和评述基于深度学习和机器学习的高光谱图像分类领域的研究进展。本综述将重点关注卷积神经网络（CNN）、循环神经网络（RNN，特别是LSTM和GRU）、图神经网络（GNN）、Transformer模型以及新兴的Mamba（状态空间模型）架构及其各种创新的混合实现在高光谱图像分类任务中的理论方法、关键技术与应用成效。

​	本综述将遵循以下逻辑结构进行组织和阐述：

​	首先，引言部分（即本章）对高光谱图像处理的重要性、广泛的应用前景、高光谱图像分类任务所面临的主要挑战进行了概述，并阐明了深度学习和机器学习方法在应对这些挑战、推动该领域发展中所扮演的核心驱动作用。

​	其次，后续章节将按照主要的方法类别进行组织，依次详细介绍和分析：

​	基于卷积神经网络（CNN）的高光谱图像分类方法（第2章）

​	基于循环神经网络（RNN）的高光谱图像分类方法（第3章）

​	基于图神经网络（GNN）的高光谱图像分类方法（第4章）

​	基于Transformer模型的高光谱图像分类方法（第5章）

​	基于Mamba（状态空间模型）架构的高光谱图像分类方法（第6章）

​	随后，第7章将对前述各类方法进行横向的比较分析与深入讨论，系统评估它们在常用的基准高光谱图像数据集上的性能表现，并探讨它们在特征提取能力、模型复杂度与计算效率、对不同数据特性的鲁棒性与适应性等方面的差异、优势与不足。

​	最后，第8章将对全文进行总结与凝练，归纳当前高光谱图像分类领域的主要研究趋势、仍然面临的关键科学问题与技术挑战，并对未来的潜在研究方向和发展前景进行展望。本综述力求做到内容全面系统、分析深入透彻、信息准确可靠，期望能为从事高光谱遥感、图像处理、模式识别及相关领域研究的科研人员和研究生提供一份有价值的学术参考资料。

 



 

# 2. 基于卷积神经网络 (CNN) 的高光谱图像分类方法

​	卷积神经网络（Convolutional Neural Network, CNN）作为深度学习领域的奠基性模型之一，因其在图像识别、目标检测、图像分割等各类计算机视觉任务中展现出的卓越性能，率先被广泛引入并应用于高光谱图像分类领域，取得了显著的研究进展与应用成效。

 

## 2.1 CNN基本原理及其在图像数据处理中的核心优势

​	CNN的核心设计思想在于模拟生物视觉皮层信息处理的层次化机制，通过构建一系列专门设计的网络层结构来自动学习输入数据的多级、抽象的特征表示。其主要的构成组件通常包括卷积层（Convolutional Layer）、池化层（Pooling Layer）和全连接层（Fully Connected Layer）[^3]。

​	卷积层：作为CNN的核心，卷积层利用一组可学习的卷积核（或称为滤波器）对输入数据（如图像或特征图）进行局部区域的加权求和运算（即卷积操作），从而提取出具有局部感知特性的特征。卷积层的两个关键特性是局部连接（Local Connectivity）和权值共享（Weight Sharing）。局部连接使得每个神经元仅与输入数据的一个小尺寸邻域（感受野）相连接，这极大地减少了网络模型的参数数量，降低了过拟合的风险；权值共享则意味着同一个卷积核在输入图像的不同空间位置上共享相同的权重参数，这使得网络能够有效地检测图像中不同位置出现的相同或相似的局部模式（如边缘、角点、纹理等），并赋予了CNN对目标平移的某种程度的不变性[^25]。

​	池化层：通常紧随卷积层之后，池化层的主要功能是进行特征降维和信息选择。它通过对卷积层输出的特征图进行下采样操作（如最大池化或平均池化），一方面可以显著降低特征图的空间维度，减少后续网络层的计算量和参数数量，另一方面也有助于增强所提取特征的鲁棒性，例如提高对微小位移、形变或尺度变化的不敏感性。

​	全连接层：在经过多层卷积和池化操作之后，网络通常会设置一个或多个全连接层。全连接层将前面各层提取到的高级抽象特征进行整合与变换，并将这些特征最终映射到预定义的输出空间，例如在分类任务中映射到各个类别的概率得分。

​	CNN通过这种精心设计的分层结构，能够从原始输入数据中以端到端的方式自动学习从低级到高级的层次化特征表示。例如，网络中的浅层卷积层可能主要学习到图像的边缘、角点、颜色、纹理等基础视觉模式，而深层网络则能基于这些低级特征组合、抽象出更复杂、更具语义信息的高级特征，直至最终形成对整个图像内容的理解[^3]。这种自动学习层次化特征的强大能力，使得CNN在图像分类、目标检测、图像分割等众多视觉任务中取得了革命性的成功，其性能显著优于依赖人工设计特征的传统图像处理方法。文献[^25]明确指出，CNN因其强大的特征提取能力，已被广泛认为是处理高光谱遥感图像的一种极具前景的特征提取器和分类器技术。

 

## 2.2 一维、二维与三维CNN在高光谱图像分类中的应用范式与演进路径

​	高光谱图像本质上是一个三维数据立方体（Data Cube），它包含两个空间维度（图像的行和列）和一个光谱维度（连续的波段）。针对高光谱图像的这一独特数据结构特性，研究者们分别探索了基于一维（1D-CNN）、二维（2D-CNN）和三维（3D-CNN）卷积操作的CNN模型在高光谱图像分类任务中的应用范式及其演进路径。

​	一维卷积神经网络（1D-CNN）: 此类方法主要将高光谱图像中每个像元的光谱曲线视为一个一维序列数据。利用一维卷积核沿着光谱维度对每个像元的光谱序列进行滑动卷积操作，从而提取光谱特征[^7]。这种方法侧重于深度挖掘像元自身的光谱信息，能够有效学习相邻波段之间的相关性以及光谱曲线的局部形状特征。然而，传统的1D-CNN通常是独立地处理每个像元的光谱信息，忽略了像元之间宝贵的空间上下文信息，这在一定程度上限制了其分类性能的提升，尤其是在地物空间分布具有较强结构性或存在显著“同物异谱”现象的复杂场景中。

​	二维卷积神经网络（2D-CNN）: 2D-CNN是在计算机视觉领域应用最为广泛和成熟的模型。将其应用于高光谱图像分类时，通常存在两种主要的策略：第一种策略是首先对高光谱数据进行降维处理（例如，使用主成分分析PCA提取若干主成分）或选择特定的若干特征波段，从而得到数量有限的二维特征图，然后将这些特征图作为多通道输入送入标准的2D-CNN模型中进行空间特征的提取与学习[^7]。第二种策略是将高光谱数据立方体的每个光谱波段视为一个独立的二维图像通道（类似于RGB图像的红、绿、蓝三个通道），然后直接输入到2D-CNN中。但这种方式如果直接应用于数百个波段的原始高光谱数据，可能会因为通道数量过多而导致模型参数量巨大，难以训练；若采用逐波段处理或简单堆叠的方式，则可能难以充分利用光谱波段间的连续性和内在相关性。虽然2D-CNN能够有效地提取图像的空间特征，但在直接应用于原始高维高光谱数据时，如何有效整合光谱维度的信息，避免信息丢失或模型过于复杂，是一个需要仔细权衡的问题[^3]。

​	三维卷积神经网络（3D-CNN）: 为了更直接、更全面地利用高光谱图像中固有的空谱联合信息，三维卷积神经网络（3D-CNN）应运而生并受到广泛关注[^3]。3D卷积核能够在空间维度（图像的宽和高）和光谱维度上同时进行卷积操作，从而能够直接从原始的三维高光谱数据立方体（或从中提取的邻域子立方块，Patches）中提取空谱联合特征。这种方式能够更好地保持光谱信息的连续性和空间信息的结构性，被认为是提升高光谱图像分类精度的一种有效途径[^18]。通过3D卷积，模型可以同时学习到地物在空间邻域内的纹理、形状等空间特征，以及在光谱序列上的吸收、反射等光谱特征，并捕捉这些特征之间的复杂相互作用。然而，3D-CNN也面临着显著的挑战：其模型参数量通常远大于相应配置的1D-CNN或2D-CNN，计算复杂度也更高，因此对计算资源（如GPU显存和算力）的要求更为苛刻，并且通常需要更多的标记训练样本来避免模型过拟合，保证其泛化能力[^9]。

​	从1D-CNN专注于光谱信息，到早期2D-CNN在降维后侧重于空间信息，再到3D-CNN力图实现真正的空谱联合特征一体化提取，这条演进路径清晰地反映了研究者在高光谱图像分类任务中不断寻求更有效利用空谱信息的持续努力。尽管3D-CNN因其直接处理三维数据立方体的能力而成为当前高光谱图像分类领域的主流趋势之一，但其计算效率和参数数量一直是研究者们致力于优化和改进的核心问题。这一挑战也直接推动了后续各种混合CNN模型、轻量化CNN架构以及与其他先进模型（如Transformer、GNN）相结合的新型网络结构的发展。

 

## 2.3 混合CNN模型架构 (例如 HybridSN, Tri-CNN) 及其性能权衡分析

​	为了在强大的特征提取能力和可接受的计算开销之间取得有效的平衡，研究者们设计并提出了多种混合型卷积神经网络（Hybrid CNN）模型。这些模型通常巧妙地结合不同维度（如1D、2D、3D）的CNN模块，或者将CNN与其他类型的网络结构（如循环神经网络RNN、注意力机制Attention Mechanism）进行集成，以期发挥各自的优势，弥补单一模型的不足。

​	HybridSN (Hybrid Spectral-Spatial Network): 由Roy等人提出[^9]，是高光谱图像分类领域一种具有代表性的混合CNN模型。该模型的核心思想是分阶段地利用不同维度的CNN来提取和融合空谱特征。具体而言，HybridSN首先利用几层3D-CNN从输入的HSI邻域小块（Patches）中提取初步的空谱联合特征，这些3D卷积层能够有效地捕获光谱信息和局部空间上下文信息。然后，将3D-CNN输出的三维特征图进行展平（Flatten）并重塑（Reshape），使其适配后续2D-CNN层的输入要求。接着，利用若干2D-CNN层对这些特征图进行进一步的处理，以学习更抽象、可能具有更大感受野的深层空间特征。最后通过全连接层完成分类任务。HybridSN通过这种3D-CNN与2D-CNN的串行结合方式，能够在有效提取空谱特征的同时，显著减少模型的总参数数量和计算复杂度，相较于纯粹使用深层3D-CNN的模型而言更为高效，也更容易训练[^9]。文献[^9]中提出的一个结合了3D-CNN、2D-CNN和双向长短期记忆网络（Bi-LSTM）的混合网络，也借鉴了HybridSN中先利用CNN提取空谱特征的思想，再由Bi-LSTM进行后续的序列信息处理。

​	Tri-CNN: 由Salah等人于2023年提出[^3]，该模型构建了一种基于多尺度3D-CNN和三分支特征融合策略的网络架构。其核心思路是首先使用具有不同参数配置（可能指卷积核大小、感受野范围或网络深度）的三个并行的3D-CNN分支从输入的高光谱数据中分别提取特征。这种多分支设计旨在从不同尺度、不同层面捕获高光谱数据中丰富多样的空谱信息。然后，将这三个不同分支提取到的特征图分别进行展平操作，并将它们在特征维度上进行拼接（Concatenate），形成一个融合了多源信息的、更具表达能力的特征向量。最后，将这个融合后的特征向量送入后续的分类器（如全连接层）进行类别判别。这种多尺度、多分支的特征提取与融合策略，旨在增强模型对不同地物类型和复杂场景的适应性与分类性能。

​	除了上述两种典型的混合CNN模型外，研究领域还涌现了其他多种具有创新性的混合CNN架构：

​	残差卷积神经网络 (Residual CNN): 例如，Yuan等人在2021年提出的模型[^25]，成功地将残差学习（Residual Learning）的思想引入到高光谱图像分类的CNN模型设计中。该模型有机地整合了2D-CNN和3D-CNN的优势，首先采用3D卷积层关注光谱特征的深度提取，并通过引入通道注意力机制（Channel Attention）对不同光谱通道的特征进行自适应的重校准和优化；随后，采用具有不同卷积核大小的2D深度可分离卷积（Depthwise Separable Convolution）来获取多尺度的空间特征，同时有效减少模型的参数量。残差连接结构的应用有助于优化深层网络的训练过程，缓解梯度消失问题，从而构建更深、性能更强的分类模型。

​	结合注意力机制的CNN: 如文献[^25]中提及的通道注意力机制，以及其他类型的空间注意力机制（Spatial Attention）或空谱联合注意力机制（Spectral-Spatial Joint Attention），这些机制可以被巧妙地嵌入到CNN架构的不同位置。注意力机制能够引导CNN模型自适应地关注输入数据中更具判别性的特征区域（空间位置）、光谱波段或特征通道，赋予不同特征以不同的权重，从而提升分类性能和模型的可解释性。

​	快速紧凑型3D-CNN (Fast and Compact 3D-CNN): 例如，Ahmad等人在2021年提出的模型[^18]，虽然其核心仍基于3D-CNN，但也通过精心设计的网络结构（例如，首先对输入数据进行iPCA降维，然后采用特定的3D-CNN和2D-CNN层组合）来有效地平衡分类精度和计算效率。这类模型可以被视为一种旨在实现高效性的混合设计思路，其目标是在保持较高分类性能的同时，显著降低模型的复杂度和运行时间。

​	这些混合CNN模型的设计核心在于模块化的组合、优化与创新。通过将不同类型、不同尺度或与其他先进机制（如注意力机制、残差连接）相结合的CNN模块进行巧妙的搭配与集成，研究者试图在保持甚至提升模型强大特征提取能力的同时，有效解决单一CNN模型（尤其是深层3D-CNN）可能存在的计算成本高昂、训练样本需求量大、对特定数据特征适应性不足等问题。这种对“参数效率”（Parameter Efficiency）和“计算效率”（Computational Efficiency）的持续追求，已成为评价CNN模型在高光谱图像分类任务中实用价值的关键指标，而不仅仅是单一地追求分类精度的极致提升。

 

## 2.4 近期代表性CNN架构创新点剖析

​	近年来，卷积神经网络（CNN）在高光谱图像分类领域的研究持续深化，涌现出一批具有代表性的新型网络架构和关键技术创新。这些进展主要围绕着进一步提升特征表达的丰富性与判别力、有效降低模型的复杂度与计算开销、以及增强模型对小样本学习场景的适应性等方面展开。

​	Ahmad, M.等研究者于2021年在《IEEE Geoscience and Remote Sensing Letters》上发表了题为“A Fast and Compact 3-D CNN for Hyperspectral Image Classification”的研究成果[^18]。该研究的核心目标是构建一种兼具快速处理能力和紧凑模型结构的3D-CNN，以应对高光谱图像分类中常见的计算资源和存储限制问题。为此，他们提出了一套精巧的策略：首先对输入的高光谱数据立方体采用增量主成分分析（iPCA）技术进行光谱维度的有效降维，此举不仅显著减少了后续网络处理的数据量和冗余信息，更重要的是保留了主要的、对分类任务贡献最大的光谱特征。经过降维处理后，数据被分割成一系列小的、可能存在重叠的三维数据块（Patches），作为后续深度学习网络的输入单元。模型的主体部分精心设计为一个包含四层3D卷积层的网络结构，旨在充分且高效地提取这些数据块中的空谱联合特征。这些由3D卷积层提取得到的三维特征图随后被展平（Flatten），并输入到后续的2D卷积层和全连接层中，进行特征的进一步抽象和最终的分类判决。该模型设计的核心创新在于实现了高效预处理与深度特征提取的有机结合，通过iPCA降维显著降低了3D-CNN的计算负担和模型复杂度。其网络结构极为紧凑，例如文献中报告其在特定数据集上的参数量仅为994,166个，远少于当时许多主流的3D-CNN模型，这使得模型不仅训练速度更快，对硬件资源（如GPU显存）的要求也更低，从而更易于实际部署和推广应用。尽管模型结构轻量，但在多个公开且广泛使用的基准高光谱图像数据集（如Salinas Valley, Indian Pines, Pavia University）上的实验结果表明，该模型均取得了与当时最先进的2D或3D CNN模型相当甚至更优的分类性能，尤其是在训练样本数量有限的挑战性条件下表现突出。此外，该模型在训练和测试时间上也展现出显著优势，充分体现了其“快速”（Fast）的设计理念和高效的计算特性。

​	另一项代表性工作是Lei, R.等研究者于2021年在《IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing》上发表的“Deep Convolutional Capsule Network (DC-CapsNet) for Hyperspectral Remote Sensing Image Classification”[^31]。这项研究创新性地将胶囊网络（Capsule Network, CapsNet）的核心思想引入到高光谱图像分类任务中，并为此设计了一种深度卷积胶囊网络（DC-CapsNet）架构。胶囊网络旨在克服传统CNN在理解图像中实体间层级关系（部分-整体关系）和对视角变化鲁棒性方面的固有局限性，它通过“胶囊”（Capsule）单元来表征特定实体的多种实例化参数（如姿态、纹理、相对位置等），并采用一种称为“动态路由”（Dynamic Routing）的机制来学习和更新胶囊之间的连接权重与信息流。在DC-CapsNet模型中，研究者巧妙地利用3D卷积操作来构建适用于处理三维高光谱数据的卷积胶囊层（Convolutional Capsule Layer），从而能够有效地提取高光谱图像中蕴含的空谱联合特征。同时，模型还集成了一个轻量级的反卷积解码器网络（Deconvolutional Decoder Network），该解码器网络不仅用于对输入数据进行重构以增强模型的泛化能力，也作为一种有效的正则化手段。该工作的主要创新点体现在多个方面：首先是3D卷积胶囊层的成功构建，它将3D卷积的局部特征提取能力与胶囊网络的动态路由机制相结合，使得模型能够同时学习光谱维度和空间维度的特征，并增强了对空谱特征表示的鲁棒性和表达能力。其次，相较于原始的胶囊网络架构，DC-CapsNet通过精心设计的卷积胶囊层，在提升性能的同时显著减少了模型的参数数量和计算成本，使其更适用于处理高维且数据量通常有限的高光谱数据。再者，其设计的解码器网络更为轻量且功能强大，通过重构输入数据，有助于模型学习到更具判别性的特征表示，并有效防止模型过拟合。尤为重要的是，在包括肯尼迪航天中心（KSC）、印第安纳松林（IN）、帕维亚大学（UP）和萨利纳斯谷（SA）在内的四个常用高光谱图像数据集上的实验结果表明，DC-CapsNet即使在训练样本数量非常有限的条件下，仍能取得较高的分类精度，充分展现了其在小样本学习方面的巨大潜力和应用价值。

​	除了上述代表性工作外，近期CNN在高光谱图像分类领域的研究还呈现出以下一些重要的发展趋势：

​	多尺度特征融合 (Multi-scale Feature Fusion): 持续是CNN研究的一个核心热点。通过设计具有不同大小感受野的卷积核、构建并行的多分支网络结构、或采用特征金字塔等策略来捕获高光谱图像中不同尺度下的空间和光谱特征，然后对这些多尺度特征进行有效的融合（如拼接、相加、注意力加权等），以更好地适应高光谱图像中地物尺寸多样性和空间结构复杂性的问题[^3]。

​	注意力机制的深度融合 (Deep Integration of Attention Mechanisms): 将各种类型的注意力机制（例如通道注意力、空间注意力、空谱联合注意力、自注意力机制的变体等）更深入、更灵活地嵌入到CNN架构的不同层级和模块中。这使得网络能够自适应地学习和判断哪些特征（例如特定的空间位置、关键的光谱波段、重要的特征通道）对于当前的分类任务更为重要，从而赋予这些特征更高的权重，以提升特征的判别能力、模型的鲁棒性以及一定程度的可解释性[^25]。

​	轻量化与高效化网络设计 (Lightweight and Efficient Design): 面对高光谱图像数据量巨大、光谱维度高以及模型在资源受限平台（如星上智能处理芯片、无人机载嵌入式系统）部署需求的日益增长等挑战，研究者们持续探索各种模型压缩技术（如网络剪枝、参数量化、低秩分解）[^32]、知识蒸馏方法、以及设计本质上更为轻量的CNN架构（例如广泛采用深度可分离卷积、组卷积、ShuffleNet中的通道混洗等高效计算单元）。

​	CNN与其他先进模型的深度结合: CNN作为一种强大的局部特征提取器，其角色正越来越多地演变为更复杂的混合深度学习模型中的一个关键基础组件。例如，CNN可以为后续的全局上下文建模模块（如Transformer、Mamba等长程依赖模型）或关系推理模块（如图神经网络GNN）提供高质量的、经过初步抽象的特征输入。这种模块化、层次化的设计思想，使得CNN能够在更宏大的框架内充分发挥其在局部感知和特征学习方面的核心优势，同时借助其他模型的特性来弥补其自身在全局上下文理解、复杂关系建模等方面的不足。

​	这些进展清晰地表明，尽管CNN是高光谱图像分类领域的基石性技术，但相关的研究并未停滞不前。通过积极借鉴计算机视觉及其他人工智能领域的先进CNN架构设计理念、深度融合新兴的机制（如注意力机制、胶囊网络等）、以及与其他类型的深度学习模型进行创新性的结合与协同优化，CNN在高光谱图像分类领域的性能表现和实际应用价值仍在不断提升和拓展。

 



 

# 3. 基于循环神经网络 (RNN) 的高光谱图像分类方法

​	循环神经网络（Recurrent Neural Network, RNN）是一类专门设计用于处理序列数据的神经网络模型。鉴于高光谱图像中每个像元所包含的光谱信息天然地呈现为按照波长顺序排列的序列结构，RNN及其重要的改进变体，如长短期记忆网络（LSTM）和门控循环单元（GRU），为高光谱图像分类任务提供了一种独特的、基于序列建模的视角和技术路径。

 

## 3.1 RNN处理序列数据的核心特性及其与高光谱数据的内在契合性

​	RNN的核心特性在于其网络内部引入的循环结构（Recurrent Connections），这种结构使得网络在处理序列中的当前元素时，能够有效地利用先前元素处理过程中所产生的隐藏状态信息（Hidden State）。这种内在的“记忆”能力使得RNN能够捕获并建模序列数据中存在的随时间（或顺序）演变的依赖关系和动态模式[^26]。

​	具体到高光谱图像，其每个像元都包含一条由数百个在光谱上连续分布的、极窄波段的反射率（或辐射亮度）值所组成的光谱曲线。这条光谱曲线可以被自然地、直观地视为一个一维序列，其中波段的排列顺序（通常是按照波长从小到大或从大到小）具有明确的物理意义，反映了地物对不同波长电磁波的吸收和反射特性。因此，RNN可以将每个像元的光谱曲线作为一个输入序列进行逐波段处理，从而学习相邻波段之间的相关性、光谱特征（如吸收谷、反射峰）的局部形状与模式，以及整条光谱曲线在不同波段范围内的动态变化趋势与全局特征[^24]。文献[^26]的研究明确提出，高光谱图像中的像元光谱向量可以被有效地看作是在光谱特征空间中一个有序且连续的光谱序列，并创新性地利用RNN来表征这种内在的序列特性，以期捕捉光谱间的相关性和不同波段间的细微变异性。文献[^24]中提出的级联RNN（Cascaded RNN）模型则旨在通过构建多层RNN结构，深入探索高光谱光谱序列中所蕴含的冗余信息和互补信息，其中第一层RNN用于学习并消除相邻光谱波段间的冗余信息，而第二层RNN则进一步学习非相邻波段间的长程互补信息。这种对光谱维度作为一种具有内在顺序的序列信息的独特处理方式，为高光谱图像的特征提取和理解提供了与传统基于卷积操作（尤其是独立处理每个波段或简单堆叠）的方法不同的视角和潜力。

 

## 3.2 LSTM与GRU网络在高光谱序列建模中的关键作用与应用

​	传统的简单RNN（Simple RNN）在处理较长的序列数据时，容易出现梯度消失（Vanishing Gradient）或梯度爆炸（Exploding Gradient）的问题，这导致其难以有效地学习和记忆序列中跨度较长的依赖关系。为了克服这一核心局限性，学术界提出了一系列改进的RNN结构，其中长短期记忆网络（Long Short-Term Memory, LSTM）和门控循环单元（Gated Recurrent Unit, GRU）是最为成功和广泛应用的两种模型，它们通过引入精巧的门控机制（Gating Mechanisms）来更好地控制信息在网络中的流动与保持。

​	长短期记忆网络（LSTM）: LSTM通过在其循环单元内部引入三个关键的门控结构——输入门（Input Gate）、遗忘门（Forget Gate）和输出门（Output Gate）——以及一个独立的细胞状态（Cell State）来实现对信息的长期记忆与选择性更新。这些门控结构使得LSTM能够根据当前输入和前序状态，有选择地决定哪些新信息需要被读取并存储到细胞状态中（输入门），哪些旧信息需要从细胞状态中被遗忘或丢弃（遗忘门），以及细胞状态中的哪些信息需要在当前时刻被输出（输出门）。这种精细的控制机制使得LSTM能够有效地学习和保持长距离的依赖关系，极大地缓解了传统RNN中的梯度问题[^16]。在高光谱图像分类任务中，LSTM常被用于逐像元地处理其光谱序列，以提取深层次的、能够反映光谱动态变化的光谱特征。例如，文献[^34]中提出的谱空LSTM网络（Spectral-Spatial LSTM Network），就是将每个像元在不同光谱通道上的反射率值逐一送入一个专门的光谱LSTM（Spectral LSTM）模块中，以学习和捕获光谱序列特征。

​	门控循环单元（GRU）: GRU可以被视为LSTM的一种流行的、结构更为简化的变体[^12]。GRU将LSTM中的输入门和遗忘门合并为一个单一的“更新门”（Update Gate），并且将细胞状态和隐藏状态进行了某种程度的混合与耦合，从而减少了门控数量和模型参数。尽管其结构相对更简单，GRU在许多序列建模任务上的表现与LSTM相当，有时甚至因为参数更少而具有更高的计算效率和更快的收敛速度。文献[^26]的研究就提出了一种带有专门设计的激活函数和改进GRU单元的RNN模型，用于解决高光谱图像的多类别分类问题，并取得了良好的效果。

​	在实际应用中，例如文献[^9]中提出的混合CNN与Bi-LSTM的网络架构，在利用CNN提取了初步的空谱特征之后，进一步采用双向LSTM（Bi-LSTM）来学习这些特征在层级间的信息传递和长程依赖关系，以增强特征的表达能力。此外，文献[^16]的综述中也提及，当前应用于高光谱图像分类（HSIC）的图循环神经网络（Graph Recurrent Neural Networks, GRNNs）主要包含基于GRU和LSTM的模型，并讨论了诸如门控图神经网络（Gated Graph Neural Network, GGNN，其内部使用GRU单元）和图LSTM（Graph LSTM）等重要的变体。总体而言，LSTM和GRU因其更强的长程依赖建模能力和对梯度问题的有效缓解机制，已成为高光谱图像光谱序列分析中RNN应用的主流选择，为从序列视角理解和利用光谱信息提供了有力的工具。

 

## 3.3 级联RNN及其他RNN变体架构的探索

​	为了进一步提升循环神经网络（RNN）在高光谱图像分类任务中的性能，并克服单一RNN模型可能存在的局限性，研究者们积极探索了多种RNN的变体结构和创新的混合模型设计。

​	级联RNN (Cascaded RNN): 正如Hang等人在文献[^24]中所提出的模型那样，通过构建多层级的RNN结构来对光谱信息进行逐步的、层次化的处理和提炼。在该模型的设计中，第一层RNN主要负责处理相邻波段之间的关系，其目标是学习并消除光谱数据中由于高度相关性而产生的冗余信息。随后，第二层RNN则在此基础上，进一步学习非相邻波段之间可能存在的长程互补信息，从而增强最终学习到的光谱特征的判别能力和信息完整性。这种分阶段、分层次处理光谱序列的思想，有助于模型更精细地捕捉光谱曲线中不同尺度和不同性质的信息。

​	双向RNN (Bidirectional RNN, Bi-LSTM/Bi-GRU): 标准的单向RNN在处理序列数据时，当前时刻的输出仅仅依赖于过去（或称前向）的输入信息。然而，在许多序列分析任务中，当前元素的理解往往也需要依赖于其后续（或称后向）的上下文信息。双向RNN通过引入一个额外的前馈层，使得网络能够同时从两个方向（前向和后向）处理输入序列。具体而言，它包含两个并行的RNN层：一个按原始顺序处理序列，另一个按相反顺序处理序列。在每个时间步，双向RNN的输出是这两个方向RNN层隐藏状态的某种组合（如拼接或相加），从而使得模型在每个时间步都能同时利用过去和未来的上下文信息，进而能够更全面、更准确地理解序列的内在含义[^9]。在文献[^9]提出的混合CNN与Bi-LSTM的网络中，Bi-LSTM模块能够从正反两个方向处理由CNN提取出来的特征序列，从而更有效地学习特征之间的相互作用和长程依赖关系。文献[^16]中也提到，Tang等人（2021年）将光谱特征输入到Bi-LSTM模型中，利用其可变的感受野来检测短程和长程的空间关系（此处应指特征序列中的关系），从而更深入地探索全局空间特征的表示。

​	混合模型 (Hybrid Models): RNN由于其主要针对一维序列数据进行设计的特性，在直接应用于高光谱图像分类时，往往难以充分有效地利用图像中至关重要的空间上下文信息。因此，将RNN与那些擅长处理空间信息的网络结构（如卷积神经网络CNN）相结合，形成CNN-RNN混合架构，是一种常见且有效的策略。在这类混合模型中，CNN通常扮演前端特征提取器的角色，负责从输入的图像数据（如像元邻域块）中提取局部的空间特征或初步的空谱联合特征。然后，将这些由CNN提取出来的特征（或其某种序列化的形式）输入到后续的RNN模块中，由RNN进一步处理这些特征序列，以捕捉光谱动态变化、特征间的时序依赖关系或更深层次的语义信息。文献[^9]在讨论HybridSN模型时，也提及了Bi-LSTM在类似的混合框架中发挥作用的潜力。文献[^9]中具体提出的3D-CNN + 2D-CNN + Bi-LSTM网络便是一个将CNN的空谱特征提取能力与Bi-LSTM的序列建模能力进行有效结合的成功范例。

​	这些RNN的变体结构和混合模型设计，其核心目标在于克服单一RNN（尤其是传统的单向简单RNN）在信息利用的充分性（例如对未来上下文信息的缺失）、对多维数据（如高光谱图像固有的空谱三维结构）处理能力的局限性以及长程依赖学习的有效性等方面存在的不足。通过这些改进和创新，RNN在高光谱图像分类中的角色，正逐渐从一个独立的光谱特征提取器，演变为更复杂的、多模态深度学习架构中一个专门负责处理序列化特征（无论是原始的光谱序列，还是由CNN或GNN等其他模型预先提取出来的特征序列）的“专家模块”，为高光谱数据的综合分析与智能解译贡献其独特的序列建模能力。

 

## 3.4 RNN方法在高光谱图像分类中的优势与固有局限性

​	循环神经网络（RNN）及其各种改进变体在高光谱图像分类任务中展现出其独特的优势，尤其是在对光谱序列信息的精细建模方面。然而，它们也存在一些固有的局限性，需要在实际应用和模型设计中加以权衡。

​	优势:

​	有效捕捉光谱序列依赖特性: RNN的核心优势在于其能够对序列数据进行有效的建模。因此，它非常适合处理高光谱图像中每个像元所呈现的光谱曲线，能够捕捉相邻波段之间的内在相关性、光谱吸收与反射特征随波长变化的动态模式，以及光谱曲线的整体形态特征[^24]。这种能力对于区分光谱特征细微但具有序列规律的地物类别尤为重要。

​	学习长程光谱依赖关系: 以LSTM和GRU为代表的门控RNN结构，通过其精巧的内部机制，能够有效地学习和记忆光谱序列中跨度较长的依赖关系。这对于识别那些依赖于光谱曲线整体形状特征或特定宽谱段组合才能区分的地物类别具有重要潜力[^16]。例如，某些矿物的诊断性吸收特征可能分布在相隔较远的不同波段区域。

​	与CNN等模型形成优势互补: 当RNN与CNN等其他类型的深度学习模型相结合构成混合架构时，可以实现优势互补。在这种组合中，RNN可以专注于处理光谱维度上的序列特性或由CNN提取出来的特征序列，而CNN则主要负责空间信息的提取和局部模式的识别，从而实现对高光谱数据空谱信息的协同、综合利用[^9]。

​	局限性:

​	空间信息处理能力相对不足: 传统的RNN模型主要是为处理一维序列数据而设计的。若直接将单个像元的光谱序列输入RNN进行处理，则会完全忽略像元周围宝贵的空间上下文信息，而这对于区分空间上聚集分布或具有特定纹理结构的地物至关重要。虽然可以通过一些策略（例如，先利用CNN提取包含空间信息的特征图，再将其序列化后送入RNN；或者构建特定的谱空RNN结构）来一定程度上缓解此问题，但其对复杂空间结构的直接建模能力通常不如CNN或GNN等模型。

​	计算效率与并行化处理的限制: RNN的循环计算特性（即当前时刻的计算依赖于前一时刻的输出）使其难以像CNN中的卷积操作或Transformer中的自注意力机制那样进行大规模的并行化处理。尤其是在处理非常长的序列（例如具有极高数量波段的高光谱数据）或构建非常深层的RNN网络时，其训练时间可能会相对较长，计算效率成为一个需要考虑的因素。

​	梯度传播问题: 尽管LSTM和GRU等门控机制通过引入“门”结构极大地缓解了传统RNN中普遍存在的梯度消失或梯度爆炸问题，但在处理极深的RNN网络或极长的序列数据时，这些问题仍有可能在一定程度上存在，从而影响模型的训练效果和收敛性能。

​	对噪声和信息冗余的敏感性: 光谱序列中存在的噪声干扰和波段之间的高度相关性（信息冗余）可能会对RNN的学习效率和最终性能产生不利影响。因此，通常需要有效的预处理步骤（如去噪、波段选择或降维）或在模型设计层面加以考虑（如注意力机制的引入）来应对这些问题。

​	图循环网络（GRNN）的特定局限: 正如文献[^16]中所述，当RNN的思想被扩展到图结构数据上形成图循环神经网络（GRNN）并应用于高光谱图像分类时，这类模型可能存在对图中边的作用区分能力较弱（即不同类型的邻居关系可能被同等对待）以及因依赖不动点理论而可能导致的“过平滑”（Over-smoothing）问题。过平滑现象会使得图中不同节点的表示趋于一致，从而削弱模型在图节点分类等任务中的判别性能。

​	总结而言，RNN在高光谱图像分类任务中为光谱特征的深度提取和精细化利用提供了一种基于序列建模的独特且有价值的思路。然而，考虑到其在空间信息直接处理能力和计算效率等方面的固有局限性，纯粹的RNN模型在当前的高光谱图像分类研究中已不常见。其更大的潜力在于作为更复杂的混合深度学习架构中的一个关键组件，与其他模型（特别是CNN和GNN）协同工作，专注于其所擅长的序列依赖建模任务，从而为高光谱数据的综合分析与智能解译贡献其独特的力量。尽管Transformer模型因其优异的并行计算能力和强大的全局感受野在序列处理任务上表现出众，但RNN（尤其是GRU/LSTM）在某些特定场景下（例如，当需要更细致的、逐步积累的信息流，或者模型轻量化是首要考虑因素时）仍有其不可替代的价值。特别是在与GNN等图结构学习模型结合形成如图循环网络（GRNN）时，其研究价值和应用前景依然存在。

 



 

# 4. 基于图神经网络 (GNN) 的高光谱图像分类方法

​	图神经网络（Graph Neural Network, GNN）作为一种专门设计用于处理和学习图结构数据的深度学习模型，近年来在高光谱图像分类领域受到了学术界的广泛关注与深入研究。GNN的核心优势在于其能够直接在非欧几里得（Non-Euclidean）空间中对数据进行操作，为建模高光谱图像中像元之间复杂的空间与光谱关系提供了全新的视角和强大的技术途径。

 

## 4.1 GNN在处理非欧氏空间数据中的独特能力与优势

​	与主要处理规则网格结构数据（例如图像中的像素栅格）的卷积神经网络（CNN）不同，图神经网络（GNN）的核心优势在于其能够直接对以“图”（Graph）形式表示的、具有非欧几里得结构的数据进行有效的学习和推理[^6]。图是由节点（Nodes 或 Vertices）和连接节点的边（Edges）所构成的数学结构，能够灵活地表示现实世界中各种实体及其之间存在的复杂关系。

​	在高光谱图像分类任务的背景下，可以将图像中的每一个像元（Pixel）或者由若干像元构成的超像元（Superpixel）视作图中的一个节点。而节点之间的边的定义则可以依据多种不同的准则来进行，例如：

​	空间邻近性: 在图像空间上相邻（如满足4邻域或8邻域条件）的像元或超像元之间建立连接边。

​	光谱相似性: 光谱特征（如光谱曲线向量）在某种度量下（如欧氏距离、光谱角、相关系数）相似的像元或超像元之间建立连接边。

​	空谱联合相似性: 综合考虑像元（或超像元）之间的空间距离和光谱相似度来共同决定是否建立边以及边的权重。

​	一旦将高光谱图像数据成功地表示为一个图结构，GNN就能够通过其核心的邻域聚合（Neighborhood Aggregation）机制来学习每个中心节点的有效表示（Embedding）。在这一过程中，GNN迭代地聚合其邻居节点的信息，并结合中心节点自身的特征来更新其表示，从而捕获图结构中所蕴含的局部模式、高阶依赖关系以及全局拓扑特性。文献[^16]的研究明确强调，GNN为高光谱图像分类提供了一个强大的半监督学习框架，能够有效地捕获和利用高光谱图像数据中固有的、非欧几里得的复杂空谱关系。文献[^35]指出，GNN能够处理基于图的数据表示，并有效地利用图中的局部和全局信息进行学习。文献[^36]和[^6]也提到，GNN能够有效地表示和分析那些超出传统基于规则网格采样约束的不规则数据，并能准确捕获高光谱图像中复杂且不规则的类别边界。这种处理不规则数据的独特能力，使得GNN在建模高光谱图像中地物复杂的空间分布格局、像元间的相互作用以及类别边界的精细刻画方面具有天然的优势，有望突破传统基于规则窗口或图像块的CNN方法在处理具有复杂形状和任意拓扑结构的地物时的固有局限性。

 

## 4.2 图卷积网络 (GCN)、图自编码器 (GAE)、图循环网络 (GRNN) 等核心模型详解

​	图神经网络（GNN）是一个涵盖了多种具体模型架构的广泛类别。在高光谱图像分类的研究与应用中，以下几种GNN模型及其变体较为常见和重要[^16]：

### 4.2.1 图卷积网络 (Graph Convolutional Network, GCN):

​	GCN 是最为流行和基础的 GNN 模型之一，其核心思想是将传统 CNN 中的卷积操作推广到图结构数据上。GCN 通过迭代地聚合每个节点的邻居节点的特征信息来更新该节点的表示（Embedding）。一个典型的 GCN 层操作可以形式化地表示为：

$$
H^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)})
$$

​	其中：
* $H^{(l)}$ 是第 $l$ 层的节点特征矩阵。
* $\tilde{A} = A + I$ 是加入了自环（Self-loops）的邻接矩阵（$A$ 为原始邻接矩阵，$I$ 为单位矩阵）。
* $\tilde{D}$ 是 $\tilde{A}$ 的（对角）度矩阵，其对角线元素 $\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}$。
* $W^{(l)}$ 是第 $l$ 层的可学习的权重参数矩阵。
* $\sigma$ 是激活函数（如 ReLU）。
* $\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$ 是对称归一化的邻接矩阵，这种归一化操作有助于稳定数值计算和改善学习性能。





​	在GCN的框架下，Hong等人于2020年在IEEE TGRS上发表的题为 "Graph Convolutional Networks for Hyperspectral Image Classification" 的工作中提出了MiniGCN [^36]。这项工作着重解决了传统GCN在处理大规模遥感图像（如高光谱图像，其节点数量可能非常巨大）时面临的计算资源和内存存储挑战。传统的GCN通常需要将整个图的邻接矩阵和特征矩阵加载到内存中进行全批量（Full-batch）训练，这在实际应用中往往是不可行的。MiniGCN通过引入小批量（Minibatch）训练机制，并结合有效的邻域采样策略，允许对大规模GCN进行分批次训练，从而显著降低了单次迭代的计算量和内存需求。更为重要的是，MiniGCN具备对训练集之外的、未在训练阶段见过的样本（Out-of-sample Data）进行有效推断（Inductive Learning）的能力，而无需重新训练整个网络模型。这一特性极大地增强了模型的实用性和可扩展性，使其能够应用于更大规模的数据集和更广泛的应用场景。该工作还系统地比较了CNN和GCN在高光谱图像分类中的性能表现，并探索了将MiniGCN与CNN进行特征级融合的多种策略（例如加性融合、元素级乘法融合和拼接融合），旨在结合两类模型在提取不同类型特征方面的各自优势，以期获得更优的分类效果。与此同时，Mou等人于2020年在IEEE TGRS上发表了Nonlocal GCN (Nonlocal Graph Convolutional Networks) [^39]。该模型的核心思想是将计算机视觉领域中用于捕捉图像长距离依赖关系的“非局部操作”（Non-local Operation）的理念引入到GCN的框架之中。传统的GCN主要通过聚合节点的局部邻域信息来更新节点表示，而Nonlocal GCN则通过计算图中任意两个节点之间的交互关系（而非仅仅是直接相连的局部邻居），使得每个节点能够感知到图中更全局化的上下文信息，从而学习到更具全局视野的节点表示。这种机制增强了模型对图像中大范围空间结构和语义关联的感知能力。Nonlocal GCN通常在半监督学习的框架下运作，能够同时有效地利用少量已标记样本和大量未标记样本进行训练，在高光谱图像分类任务中取得了具有竞争力的分类结果和高质量的分类图。



### 4.2.2 图自编码器 (Graph Autoencoder, GAE):

​	GAE是一种主要用于学习图中节点低维嵌入表示（Node Embeddings）的无监督或半监督学习模型[^16]。它通常由一个编码器（Encoder）和一个解码器（Decoder）两部分组成。编码器（通常采用GCN结构）负责将输入的图结构信息（如邻接矩阵）和节点原始特征映射到一个低维的、稠密的潜在表示空间（Latent Space）。解码器则尝试从这些学习到的潜在表示中重构原始图的某些属性，例如重构图的邻接矩阵（即预测节点间的连接关系）或节点自身的特征。通过最小化重构误差（例如邻接矩阵的重构损失或节点特征的重构损失），GAE能够学习到既能捕捉图的拓扑结构信息又能保留节点属性信息的有效嵌入表示。这些学习到的节点嵌入可以被用于后续的各种下游任务，如节点分类、图聚类、链接预测等。



### 4.2.3 图循环网络 (Graph Recurrent Neural Network, GRNN):

​	GRNN是将循环神经网络（RNN）的核心思想扩展到图结构数据上的一类模型[^16]。在GRNN中，每个节点的状态会通过与其邻居节点进行信息的交换和交互来迭代地更新，这个过程会持续进行，直到所有节点的状态达到一个稳定或平衡点，或者达到预设的迭代次数。这种迭代更新的过程在概念上类似于RNN中信息沿着时间序列的传播和演化。GRNN特别适用于捕获图中节点状态的动态演化过程或序列依赖关系。文献[^16]的综述中详细讨论了基于GRU和LSTM的GRNN模型及其在高光谱图像分类（HSIC）中的应用潜力，例如门控图神经网络（Gated Graph Neural Network, GGNN，其内部使用GRU单元来控制信息的更新和传递）和图LSTM（Graph LSTM）等变体。



### 4.2.4 混合GNN (Hybrid GNNs):

​	为了充分利用不同类型深度学习模型的独特优势，研究者们常常将GNN与其他模型（尤其是CNN）进行有机的结合，从而构建出混合GNN架构（Hybrid GNN Architectures）[^16]。这些混合模型通常采用串行或并行的方式进行特征的提取与融合。例如，在一个典型的CNN-GNN混合模型中，CNN可以首先从高光谱图像的像元邻域块（Patches）中提取局部的、具有平移不变性的空谱特征；然后，将这些由CNN提取出来的特征作为GNN中对应节点的初始属性（Initial Node Features）；最后，再由GNN进一步学习这些节点（代表像元或超像元）在图结构上的高阶关系和全局上下文信息，从而实现对局部特征和全局关系的协同建模。文献[^39]中列举并讨论了多种CNN与GCN的结合方法，例如MFGCN（Multi-Feature Fusion GCN，多特征融合GCN）、CEGCN（CNN-Enhanced GCN，CNN增强的GCN）以及WFCG（Weighted Feature Fusion of CNN and Graph Attention Network，CNN与图注意力网络加权特征融合）等。

​	总结而言，GCN因其原理的直接性、实现的相对简洁性以及在多种图学习任务上展现出的有效性，成为目前高光谱图像分类中应用最为广泛和深入研究的GNN模型。GAE和GRNN则在特定的无监督特征学习、图表示学习或动态关系建模场景下展现出其独特的潜力。而混合GNN架构，特别是CNN与GNN的深度结合，是当前研究的一个重要热点和发展趋势，其核心目标在于实现CNN强大的局部特征提取能力与GNN灵活的全局关系建模能力的优势互补与协同增强。GNN在高光谱图像分类领域的兴起，本质上是对传统基于规则网格处理方法（如纯CNN）在处理高光谱数据时遇到的一些瓶颈问题（例如难以有效建模不规则的地物边界、难以捕捉复杂的空间上下文关系、对旋转和尺度变化不够鲁棒等）的一种积极响应和重要的技术突破。

 

## 4.3 GNN在捕捉空谱联合特征与全局上下文信息方面的进展

​	图神经网络（GNN）通过其灵活多样的图构建过程和独特的邻域信息聚合机制，为高光谱图像的空谱联合特征提取以及全局上下文信息的有效利用提供了一个强大且富有潜力的分析框架。

​	图的构建（Graph Construction） 是将GNN应用于高光谱图像分类的首要步骤，也是将空间信息和光谱信息进行有效融合的关键环节。在实践中，研究者通常会采用以下几种核心策略来构建能够准确反映高光谱图像内在结构特性和地物关系的图：

​	基于像元的图构建: 在这种策略中，将高光谱图像中的每一个像元（Pixel）都视作图中的一个节点（Node）。节点之间的边（Edge）可以依据多种准则来定义和加权。例如，可以仅根据像元在图像网格上的空间邻近关系（如满足4邻域或8邻域连接性）来建立边；也可以根据像元光谱向量之间的相似性（例如计算光谱角、欧氏距离或相关系数等）来定义边，将光谱特征相似的像元连接起来；更常见的是综合考虑空间邻近性和光谱相似性，例如只在空间相邻的像元之间，根据它们的光谱相似度来赋予边的权重。这种基于像元的图构建方式能够最细致地保留原始数据的空间和光谱信息，但当图像尺寸较大时，生成的图规模也会非常庞大，对计算资源和存储空间提出较高要求。

​	基于超像元的图构建: 为了有效降低图的规模和后续GNN模型的计算复杂度，一种常用的方法是先对高光谱图像进行超像元分割（Superpixel Segmentation），例如采用简单线性迭代聚类（SLIC）算法[^17]或其他分割技术，将原始图像分割成若干个空间上连续且光谱特征相对均质的小区域（即超像元）。然后，将每一个超像元视作图中的一个节点。超像元的特征可以由其内部所有像元光谱特征的某种聚合（如平均光谱、中位数光谱或更复杂的特征描述子）来表示。超像元之间的边的定义方式与基于像元的图类似，同样可以基于它们在空间上的邻接关系或其聚合特征之间的相似性来确定。这种基于超像元的方法在一定程度上平滑了图像中的噪声，并利用了区域内像元的光谱一致性，同时显著减少了图的节点数量。

​	在图构建完成之后，GNN的核心操作——邻域聚合（Neighborhood Aggregation）（如图卷积层所执行的）——能够自然地、迭代地融合节点的空间信息和光谱信息。在GNN的每一层中，中心节点的特征更新不仅依赖于其自身在上一层学习到的特征表示（这些初始特征通常是原始像元的光谱信息，或者是经过CNN等模型预提取的空谱特征），还依赖于其在图结构上连接的邻居节点的特征表示，以及它们之间连接边的属性（如边的权重，可以反映邻居节点的重要性或关系的强度）。通过堆叠多层GNN结构，信息可以在图中进行有效的远距离传播和扩散，使得每个节点能够感知到更大范围的上下文信息，从而学习到更具全局视野和判别能力的特征表示。

​	许多近期的GNN模型都明确地设计用于提取和增强空谱联合特征。例如，文献[^35]中提到的谱空GCN（Spectral-Spatial GCN）就是直接在一个同时考虑了光谱信息和空间邻接关系的图上进行卷积操作。文献[^39]中列举并分析的多种CNN与GCN的混合模型，如MFGCN（多特征融合GCN）、CEGCN（CNN增强的GCN）、WFCG（CNN与图注意力网络加权特征融合）等，其核心思想都是先利用CNN强大的局部特征提取能力，从原始高光谱数据中提取出高质量的局部空谱特征，并将这些特征作为GNN中对应节点的初始属性表示，然后再由GCN来学习这些特征在图结构上的高阶关系和全局分布模式。文献[^37]提出的并行GNN网络（PGNN-Net）则采用了改进的GCN和ChebNet（一种基于谱图理论的图卷积网络）并行地从不同的角度和层面提取局部和全局的空谱特征，然后对这些多源特征进行有效的融合。这些研究进展清晰地表明，GNN通过其灵活的图表示能力和强大的关系学习机制，为高光谱图像的空谱信息深度融合提供了一个能够超越传统CNN基于规则网格限制的、更具潜力的强大框架。

 

## 4.4 多尺度分析、动态图构建及注意力机制在GNN中的深度融合

​	为了进一步提升图神经网络（GNN）在高光谱图像分类任务中的性能表现，并有效克服传统GNN模型可能存在的一些局限性（例如，图的构建方式对最终结果的敏感性、固定的邻域聚合策略难以适应复杂多变的数据特性等），研究者们积极地将多尺度分析（Multi-scale Analysis）、动态图学习（Dynamic Graph Learning）以及注意力机制（Attention Mechanisms）等先进的机器学习技术与思想融入到GNN模型的设计之中。

​	在高光谱图像中，地物目标往往展现出多样的空间尺度和结构特征。为了有效捕获这些在不同尺度上表现出来的地物信息和上下文关系，研究者们提出了基于多尺度图的GNN模型[^17]。实现这一目标通常有几种途径，例如利用超像元分割算法（如SLIC），通过调整参数（如初始聚类中心的数量或紧凑度因子）生成一系列具有不同平均尺寸和粒度的超像元图，然后在这些不同尺度的图上分别应用GNN进行特征学习，并最终融合从各尺度提取的特征[^17]；或者，在图构建或GNN的邻域聚合阶段，为每个节点定义不同大小或不同跳数（Hops）的邻域范围，从而使GNN能够感知和学习不同尺度的感受野信息。

​	在这一方向上，Wang, C.等人于2024年在《Canadian Journal of Remote Sensing》发表的题为“Multi-Scale Dense Graph Attention Network for Hyperspectral Classification”的研究[^17]，提出了一种名为MSDesGATnet的模型。该模型针对单一固定图结构难以充分适应高光谱图像复杂特征表示的问题，设计了一种多尺度稠密连接的图注意力网络架构。其核心创新在于，首先利用SLIC算法通过改变预设的聚类中心数量来生成多尺度的超像元图；接着，引入了一个新颖的图节点特征对齐（Graph Node Feature Alignment, GNFA）模块，该模块通过一种像素-节点-像素级的序列变换操作，实现了不同尺度图之间节点特征的有效对齐和跨尺度信息连接，从而成功地将多尺度图的思想与DenseNet（稠密连接网络）的架构进行了有机结合。在每个单一尺度的图内部，则采用改进的图注意力网络（GATv2）进行节点特征的提取和聚合。这种精心设计的多尺度稠密连接结构旨在生成指数级增长的、更加丰富的特征图，以增强模型对节点（超像元）内部可能存在的类别不纯（杂质）现象的鲁棒性，并有效地融合来自不同空间尺度的上下文信息。据作者称，这是首次将多尺度图与DenseNet架构集成并应用于高光谱图像分类任务。与此同时，Wan, S.等人在2020年（在线发表于2019年）的《IEEE Transactions on Geoscience and Remote Sensing》上提出的Multiscale Dynamic Graph Convolutional Network (MDGCN) [^39]，则致力于解决传统GCN在处理高光谱图像时图结构固定不变、难以适应地物不规则分布和复杂边界的固有问题。为此，MDGCN创新性地引入了动态图构建机制，即图的邻接关系（节点间的连接及其权重）并非预先固定和静态定义的，而是在模型训练的过程中根据节点特征的相似性进行动态地调整和更新。同时，MDGCN有机地结合了多尺度思想，通过在不同邻域范围（例如，通过改变k近邻算法中的k值来定义不同大小的邻域）构建图并进行相应的图卷积操作，从而能够有效地捕获不同尺度的空间上下文信息。这种动态图构建与多尺度特征学习的协同机制，使得模型能够更好地适应高光谱图像中地物形状和大小的多样性及复杂性，并在多个公开的基准数据集上取得了优于当时其他主流方法的分类性能。

​	与MDGCN模型的核心思想类似，动态图（Dynamic Graphs）学习的理念是允许图的拓扑结构（即节点之间的连接关系和边的权重）在模型训练过程中根据数据本身的特性进行学习和自适应调整，而不是依赖于一个固定的、预先定义的图结构[^39]。这使得GNN能够更好地发现数据中潜在的、与当前学习任务更相关的关联模式，从而构建出更优化的图表示，提升模型的性能。此外，将注意力机制（Attention Mechanisms）引入到GNN模型中，可以使模型在聚合邻域信息时，能够为不同的邻居节点分配不同的重要性权重。这意味着模型可以根据中心节点和其邻居节点的特征，自适应地、有选择地关注那些对当前中心节点表示更新更重要、信息量更大的邻居节点，同时抑制来自不相关或噪声邻居的干扰。图注意力网络（Graph Attention Network, GAT）是这一方向的典型代表[^17]。GAT通过引入自注意力（Self-Attention）机制来计算邻居节点相对于中心节点的注意力系数，然后利用这些注意力系数对邻居节点的特征进行加权聚合，形成中心节点新的表示。文献[^17]中提出的MSDesGATnet模型就采用了GATv2（GAT的一个改进版本，旨在解决原GAT中注意力权重静态的问题）。文献[^50]也提到了一种基于局部光谱滤波的图注意力网络，这表明注意力机制与GNN的结合是提升模型性能和可解释性的重要途径。

​	值得强调的是，“图构建”本身已成为GNN在高光谱图像分类应用中的一个核心研究问题，其构建的质量直接且深刻地影响着最终的分类性能。如果图的构建方式不佳，例如错误地连接了本不相关的像元（或超像元），或者遗漏了关键的像元（或超像元）之间的连接关系，那么即使后续采用的GNN模型本身再强大，其性能也会受到极大的限制。因此，动态图学习、多尺度图构建以及结合了注意力机制的图学习方法，其共同目标都是为了让图的结构能够更准确、更自适应地反映高光谱数据内在的复杂关联特性，从而提升GNN的特征学习能力和最终的分类精度。这些先进机制的引入，显著增强了GNN模型在高光谱图像分类任务中的表达能力和对数据复杂特性的适应性。同时，正如MiniGCN[^38]的一个重要贡献是使其能够以小批量方式进行训练，从而方便其与CNN进行有效的特征级融合一样，GNN与CNN的深度融合已成为一种主流的研究趋势。这种结合旨在充分发挥CNN强大的局部特征提取能力和GNN灵活的关系建模与全局信息聚合能力的双重优势，有望在未来的研究中催生出更为强大、更为鲁棒的高光谱图像智能分类模型。

 



 

# 5. 基于Transformer的高光谱图像分类方法

​	Transformer模型最初是在自然语言处理（Natural Language Processing, NLP）领域取得了革命性的成功，其核心的自注意力（Self-Attention）机制赋予了模型前所未有的捕捉和建模数据长距离依赖关系的能力。近年来，研究者们成功地将Transformer的强大思想和核心机制迁移并应用于计算机视觉领域，并在高光谱图像分类这一具有挑战性的任务中展现出巨大的潜力与应用前景。

 

## 5.1 Transformer模型的核心机制 (自注意力) 及其在视觉任务中的巨大潜力

​	Transformer模型的核心驱动力在于其自注意力机制（Self-Attention Mechanism）[^5]。对于输入序列中的每一个元素（例如，在NLP中是一个词的嵌入向量，在视觉中可能是一个图像块的特征向量），自注意力机制会计算该元素与序列中所有其他元素（包括其自身在内）之间的相关性得分，这些得分随后被归一化处理（通常通过Softmax函数）以形成注意力权重（Attention Weights）。然后，根据这些动态计算得到的注意力权重，对序列中所有元素的表示进行加权求和，从而得到该元素在当前上下文中的新的、信息更丰富的表示。这个过程允许模型在更新每个元素的表示时，能够动态地、有选择地关注输入序列中的其他相关部分，无论它们在序列中的物理距离有多远。这种全局信息交互的能力使得Transformer能够非常有效地捕捉输入数据中的长距离依赖关系和全局上下文信息，克服了传统RNN在处理长序列时信息逐级传递可能导致的遗忘问题，以及传统CNN因局部感受野限制而难以直接建模全局关系的局限性。

​	当Transformer模型被应用于视觉任务时，通常需要将输入的图像数据（通常是二维或三维的）转换为一种类似于序列的一维形式，以便适配Transformer编码器的输入要求。Vision Transformer (ViT)[^5] 是一项具有里程碑意义的开创性工作，它成功地将标准的Transformer架构应用于图像分类任务。ViT首先将输入的二维图像分割成一系列不重叠（或有少量重叠）的小图像块（Patches），然后将这些图像块线性展平（Flatten）成一维向量，并通过一个可学习的线性投影层将其映射到合适的特征维度。为了保留图像块的空间位置信息（因为Transformer本身不具备处理序列顺序的能力），ViT会为每个图像块的嵌入向量添加一个可学习的位置编码（Positional Encoding）。最后，将这些带有位置信息的图像块嵌入序列视为一个句子中的词序列，输入到标准的Transformer编码器中进行特征学习和分类。ViT及其后续的一系列变体模型在图像分类、目标检测、图像分割等多种主流计算机视觉任务上都取得了与基于CNN的SOTA模型相当甚至更优的性能，这充分证明了Transformer架构在视觉领域学习全局特征表示、捕捉长程依赖关系方面的强大潜力。文献[^52]指出，Transformer框架能够很好地表示图像中的高级语义特征。文献[^53]则进一步强调，Transformer通过其核心的自注意力机制，能够模仿生物视觉系统中的显著性检测和选择性注意能力，从而有效地建立输入元素之间的长距离依赖关系，这对于解决传统CNN因其固有的局部感受野限制而难以有效捕捉全局信息的关键问题至关重要。对于高光谱图像而言，其数据既包含二维的空间结构信息，又包含长达数百个波段的光谱序列信息，Transformer模型所具备的全局感受野和动态权重分配能力，使其非常适合于处理高光谱图像中复杂的空谱关联特性和长程依赖模式。

 

## 5.2 Vision Transformer (ViT) 及其变体在高光谱图像分类中的应用探索

​	受到Vision Transformer (ViT) 在自然图像处理领域取得巨大成功的鼓舞，研究者们开始积极探索将ViT及其各种改进变体应用于高光谱图像分类任务。在这一过程中，核心的挑战在于如何有效地将高光谱图像独特的三维（空间-空间-光谱）数据结构转换为Transformer模型能够高效处理的二维（序列长度 × 特征维度）输入形式——即所谓的**标记化（Tokenization）**过程——同时在这个转换过程中最大限度地保留和利用高光谱图像中丰富的空谱联合信息。

​	根据标记化策略以及对空谱信息侧重点的不同，可以将ViT及其变体在高光谱图像分类中的应用范式大致归纳为以下几种主要方式：

​	基于光谱序列的Transformer: 此类方法主要将每个像元的光谱曲线（或其经过预处理的表示）视为一个独立的序列输入到Transformer模型中进行处理。例如，Hong等人提出的SpectralFormer[^5]模型，它将高光谱数据沿着光谱维度进行组织和处理。该模型设计了一种分组光谱嵌入（Groupwise Spectral Embedding, GSE）层，用于学习光谱子序列（即由若干相邻波段组成的光谱组）的有效表示，然后将这些表示输入到Transformer编码器中，以学习不同光谱组之间以及整个光谱曲线内部的长程依赖关系。这种方法的核心侧重点在于深度挖掘和利用光谱信息。

​	基于空间块的Transformer: 这种方式更接近于原始ViT的设计理念。它通常首先对高光谱图像进行一定的预处理（例如，通过PCA进行光谱降维后得到若干主成分图像，或者直接选择少数几个具有代表性的波段构成一个多通道的二维图像），然后将这个（或这些）二维图像在空间上分割成一系列不重叠或有少量重叠的小图像块（Patches）。这些空间图像块随后被序列化（例如，展平成一维向量并添加位置编码）后输入到Transformer模型中。这种方法更侧重于对图像空间上下文信息的建模。例如，HSI-BERT[^49] (He et al., 2020) 就是一个早期的尝试，它借鉴了NLP领域中BERT（Bidirectional Encoder Representations from Transformers）的思想，使用预训练的双向编码器Transformer结构进行高光谱图像分类，其关注点主要在于像元之间的空间依赖关系。

​	基于空谱联合特征的Transformer: 这是目前高光谱图像分类领域中更为主流且被证明更为有效的一种方式。这类方法通常首先利用卷积神经网络（例如1D-CNN, 2D-CNN, 或更常见的3D-CNN）从原始高光谱数据（通常是像元邻域的小立方块）中提取局部的、初步的空谱联合特征。然后，将这些由CNN提取出来的特征图进行进一步的标记化处理（例如，可以将特征图在空间维度上分割成更小的块，每个块展平成一个token；或者将每个空间位置上的多通道特征向量直接视为一个token），再将这些表示空谱信息的token序列输入到Transformer编码器中，进行全局上下文建模、长距离依赖关系学习以及特征的深层增强与精炼。

​	Sun, L.等人于2022年在《IEEE Transactions on Geoscience and Remote Sensing》上发表了题为“Spectral–Spatial Feature Tokenization Transformer for Hyperspectral Image Classification”的研究，提出了SSFTT模型[^5, 58]。该模型的核心目标在于有机地结合卷积神经网络（CNN）在提取局部细节特征方面的固有优势和Transformer在学习全局依赖关系方面的强大能力。为实现这一目标，SSFTT首先构建了一个谱空特征提取模块，该模块通常由若干3D卷积层和2D卷积层串联组成，用于从输入的HSI数据块中高效提取浅层的、局部的谱空联合特征。在此基础上，模型创新性地引入了一个高斯加权特征标记器（Gaussian Weighted Feature Tokenizer）。该标记器负责对由CNN模块提取出的特征图进行有效的转换和标记化处理，其设计旨在使得生成的token序列不仅信息丰富，而且更具可分性和代表性，为后续Transformer的全局建模奠定良好基础。这些经过精心标记化的特征token随后被送入标准的Transformer编码器模块中，进行深层次的特征表示学习和全局上下文信息的精确建模。最终，通过一个简洁的线性层对Transformer编码器输出的第一个可学习标记（通常是专门为分类任务设计的类别标记，即Class Token）进行识别，从而得到最终的分类结果。在多个常用的标准高光谱图像数据集上进行的广泛实验结果表明，SSFTT模型在分类性能上显著优于多种当时的SOTA（State-of-the-Art）方法，并且在保持高性能的同时，展现出相对较少的计算时间，体现了其在精度和效率方面的良好平衡。

​	Qing等人引入的Self-Attention-based Transformer Network (SAT-Net) [^53]，其核心是使用多个堆叠的Transformer编码器来直接从高光谱数据中提取图像特征。为了促进深层网络的有效训练并解决可能出现的梯度消失和过拟合问题，SAT-Net在这些Transformer编码器之间引入了多级的残差连接结构。Convolutional Meets Transformer Network (CMTNet) [^10] 的设计思路通常体现为CNN和Transformer的并行双分支结构（Dual-branch Architecture）。在这种架构中，一个分支利用CNN（例如3D-CNN或2D-CNN）专注于提取高光谱图像的局部谱空特征，以精确捕捉图像的细节信息和局部模式。与此同时，另一个并行分支则采用Transformer（通常是其编码器部分）来学习和捕获全局范围内的谱空特征以及数据中存在的长距离上下文依赖关系。这两个分支独立提取的不同层面、不同性质的特征，随后会在网络的某个特定阶段进行有效的融合，融合策略可以包括简单的特征拼接、逐元素相加，或是更为复杂的基于注意力的加权融合机制。此外，这类模型为了进一步提升性能，通常还会包含一个初始的谱空特征提取模块（例如通过几层卷积实现）用于捕获浅层的原始特征，以及一个多输出约束模块。该约束模块通过在网络的不同特征层级之间施加额外的监督信号或交叉约束，旨在进一步增强模型的分类精度和整体鲁棒性。这种并行双分支的设计理念，其核心目标在于更灵活、更全面地结合局部信息和全局信息，并试图解决传统CNN-Transformer串行堆叠方式中可能存在的特征交互不足或信息传递瓶颈等问题。进一步地，Xi, B.等人针对火星高光谱图像分类任务，于2025年在《IEEE Transactions on Geoscience and Remote Sensing》上发表了题为“MCTGCL: Mixed CNN-Transformer for Mars Hyperspectral Image Classification With Graph Contrastive Learning”的研究，提出了MCTGCL模型[^13]。这是一个精心设计的复杂混合模型，其架构中包含一个信息增强注意力模块（Information-Enhanced Attention Module, IEAM），该模块的核心功能是从多个不同的角度和层面聚合注意力特征，从而有效增强最终特征表示的丰富性和表达能力。模型的核心部分是一个轻量级的双分支CNN-Transformer（Lightweight Dual-branch CNN-Transformer, LDCT）网络。LDCT网络旨在高效地从高光谱数据中提取局部的精细特征和全局的上下文特征，同时在设计上努力降低模型的整体复杂度和计算开销，以适应可能的资源限制。更为关键的是，MCTGCL模型创新性地引入了图对比学习（Graph Contrastive Learning）的策略。该策略被应用于已标记样本的特征所构成的图拓扑结构之上，通过在特征空间中最大化同类样本特征之间的相似性、同时最小化异类样本特征之间的相似性，来进一步增强不同类别特征之间的判别性和可分性，从而提升分类的准确度。为了充分验证该模型的有效性和实用性，研究团队还专门为此任务标注并公开了三个新的火星高光谱图像数据集，统称为HyMars，为相关研究提供了宝贵的数据资源。

​	文献[^53]的综述中还提及了其他一些值得关注的Transformer变体模型，例如空间和光谱注意力机制融合网络（SSAMF）以及组感知层次Transformer（Group-Aware Hierarchical Transformer, GAHT）。这些多样化的研究工作共同反映了学术界在如何更好地将Transformer模型的强大全局建模能力适配到高光谱图像数据独特的空谱特性上所做出的持续努力和积极探索。ViT及其各种变体在高光谱图像分类中的应用是当前该领域的一个重要研究热点，其核心技术挑战在于如何设计出能够有效保留和深度融合空谱信息，同时生成长度适中、信息量高度浓缩的token序列的方法，以及如何构建更为高效、更具针对性的注意力机制和网络结构，以期在提升分类性能的同时，兼顾模型的计算效率和可解释性。

 

## 5.3 Transformer与CNN等主流结构的混合模型设计策略

​	尽管Transformer模型在全局特征建模方面展现出卓越的性能，但原始的Vision Transformer (ViT)及其直接应用也存在一些固有的不足之处。例如，由于其将图像分割成相对独立的图像块进行处理，可能对图像的局部细节信息不够敏感，缺乏CNN所具有的强烈的局部归纳偏置（Inductive Bias）；此外，Transformer模型通常参数量较大，往往需要海量的训练数据才能达到良好的性能，在小样本高光谱图像分类场景下容易出现过拟合。相比之下，卷积神经网络（CNN）在提取图像的局部特征（如边缘、纹理、角点等）方面非常高效且鲁棒，并且对训练数据量的要求相对较低。因此，将CNN的局部特征提取优势与Transformer的全局上下文建模能力相结合，构建混合的CNN-Transformer模型（Hybrid CNN-Transformer Models），已成为当前高光谱图像分类领域一个主流且富有成效的研究方向[^5]。

​	这些混合模型的设计策略多种多样，但其核心目标都是实现两种架构的优势互补。主要可以归纳为以下几种常见的构建方式：

​	串行结构 (Sequential Architecture): 这是最为常见和直观的一种混合方式。在这种结构中，CNN模块通常作为网络的前端（Frontend）或特征提取主干（Backbone），负责从输入的原始高光谱数据（或其邻域子块）中提取浅层的、局部的空谱特征。这些由CNN输出的特征图（Feature Maps）随后被进行“标记化”（Tokenized）处理，即将其转换成适合Transformer编码器处理的序列形式。例如，可以将二维或三维的特征图在空间维度上分割成一系列不重叠或有少量重叠的小块，然后将每个块展平（Flatten）为一个一维的特征向量（Token）；或者，也可以将特征图上每个空间位置处的多通道特征向量直接视为一个Token。然后，这个由一系列Token组成的特征序列被输入到后续的Transformer编码器中，由Transformer负责进行全局上下文信息的建模、长距离依赖关系的捕捉以及特征的进一步抽象和增强。前述的SSFTT[^52]模型就是这种串行结构的典型代表。这种设计哲学体现了一种“分工与协作”的思想：CNN负责高效地完成其擅长的局部特征提取任务，为Transformer提供更高质量、更具语义信息的输入，同时也可能通过对原始数据进行初步的降维和抽象，间接减少了Transformer需要处理的原始序列长度，从而在一定程度上降低了计算复杂度。

​	并行/双分支结构 (Parallel/Dual-Branch Architecture): 在这种设计策略中，CNN和Transformer作为两个相对独立的、并行的分支，分别从输入数据中提取不同层面或不同性质的特征。例如，CNN分支可能专注于提取高光谱图像的精细局部空间特征或局部的空谱联合特征，而Transformer分支则可能更侧重于学习全局的依赖关系、光谱序列的长程相关性或更大范围的空间上下文信息。这两个并行分支所提取出来的特征在网络的某个中间阶段或末端进行有效的融合，例如通过简单的拼接（Concatenation）、逐元素相加（Element-wise Addition）、或更复杂的注意力加权融合机制（Attention-based Fusion），然后将融合后的特征送入后续的网络层进行最终的分类判决。前述的CMTNet[^53]和MCTGCL模型中的LDCT网络[^65]都采用了类似双分支并行处理的设计思路。这种并行结构允许模型同时从不同的角度、不同的尺度捕捉和处理信息，并可能通过精心设计的特征融合机制实现更全面、更鲁棒的信息整合与表达。

​	将卷积操作融入Transformer模块内部 (Integrating Convolutional Operations into Transformer Blocks): 另一种更深层次的融合思路是将卷积操作直接嵌入到Transformer的基本构建模块（如自注意力层或前馈网络层）的内部，以增强其对局部信息的感知和处理能力，弥补标准Transformer在这方面的不足。例如，可以在自注意力计算之前或之后加入少量的卷积层（如深度可分离卷积）来对输入的Token序列进行局部特征增强；或者，可以将多头自注意力机制中的线性投影操作（Linear Projections for Query, Key, Value）替换为卷积操作，从而引入卷积的局部性和权值共享特性。Yang等人提出的高光谱图像Transformer (Hyperspectral Image Transformer, HIT)[^5] 就是一个将卷积操作集成到Transformer框架内部的例子，它旨在通过这种方式有效捕获光谱间的细微差异并增强局部空间信息的表示能力。

​	文献[^53]明确指出，CNN-Transformer混合架构通常采用卷积层来提取图像的浅层局部特征，而Transformer模块则用于提取深层次的全局特征，或者在某些设计中直接将两者的输出特征进行相加融合。文献[^5]也详细描述了CNN-Transformer联合模型是当前高光谱图像分类领域的主流架构之一，并列举了多种具体的混合方式和成功案例。这些混合模型的设计核心在于如何最好地发挥CNN在局部特征提取方面的固有优势和Transformer在全局上下文建模方面的强大能力，以期在高光谱图像分类任务中达到更优的分类性能和更好的数据利用效率。当前研究的重点在于设计更为优化的接口（即如何进行高效且信息无损的Tokenization）、更为有效的特征融合机制（如何实现局部与全局特征的深度交互与互补增强），以及如何在CNN和Transformer两个模块之间实现信息的双向流动和协同优化，从而构建出真正意义上“1+1>2”的强大混合模型。

 

## 5.4 Transformer在全局特征建模方面的优势与计算效率的权衡考量

​	Transformer模型在全局特征建模方面的核心优势源于其自注意力（Self-Attention）机制，这一机制赋予了模型前所未有的能力来捕捉输入数据中长距离的依赖关系。然而，这种强大的能力也伴随着较高的计算复杂度，因此其计算效率一直是研究者们关注和试图优化的重点问题。

​	优势:

​	全局感受野 (Global Receptive Field): 自注意力机制使得Transformer能够直接计算输入序列中任意两个元素（Tokens）之间的相互关系，无论它们在序列中的物理距离有多远。这意味着在Transformer的每一层中，网络输出的每一个元素都可能受到输入序列中所有其他元素的影响，从而使得模型天然地拥有全局的感受野。这对于捕捉高光谱图像中可能存在的长距离空间依赖关系（例如，大片同质区域内部的细微光谱变化、不同地物类型之间的复杂空间相互影响）或光谱曲线的整体形状模式与长程相关性至关重要[^5]。

​	动态权重分配 (Dynamic Weighting): 在自注意力机制中，元素之间的注意力权重是根据输入数据本身动态计算得到的，而不是像CNN中的卷积核那样具有固定的权重。这意味着模型可以根据当前的任务需求和输入内容的具体情况，自适应地决定哪些信息片段对于当前元素的表示更新更为重要，并赋予它们更高的权重。这种高度的灵活性使得Transformer能够更好地适应数据中复杂多变的模式和内在结构。

​	良好的并行计算能力: 尽管自注意力机制内部的计算量（尤其是对于较长序列）相对较大，但Transformer模型的整体结构（特别是其编码器和解码器中的多头注意力机制可以并行计算不同的注意力“头”，以及层与层之间的前馈网络部分）相比于传统的循环神经网络（RNN）更容易进行大规模的并行化处理。这有助于在现代高性能计算硬件（如GPU、TPU）上实现高效的训练和推理。

​	计算效率考量:

​	二次复杂度 (Quadratic Complexity): 标准的自注意力机制的计算复杂度和内存消耗都与输入序列长度的平方（即 O(N2)，其中 N 是序列的长度）成正比[^12]。当处理高分辨率的图像（这会导致分割出大量的空间图像块或Tokens）或具有非常长光谱序列的高光谱数据时，这个平方复杂度会迅速成为计算的瓶颈，导致训练时间过长和GPU内存不足的问题。

​	应对策略与优化方向: 为了有效解决Transformer模型在处理长序列时面临的计算效率问题，研究者们已经提出了多种行之有效的优化策略和改进模型：

​	轻量化Transformer变体: 例如，采用稀疏注意力（Sparse Attention）机制，只计算部分元素之间的注意力关系，而不是所有元素对之间的关系；或者发展线性化注意力（Linearized Attention）方法，如Performer、Linformer等，它们通过数学变换或近似将自注意力的复杂度从 O(N2) 降低到接近线性的 O(N)；再比如采用窗口化注意力（Windowed Attention）机制，如Swin Transformer[^52]模型，它将自注意力的计算限制在图像的局部窗口内部，并通过窗口间的移位操作（Shifted Window）来实现跨窗口的信息交互，从而在保持全局建模能力的同时显著降低计算量。

​	与CNN的有效结合: 正如前文所述，通过先利用CNN进行有效的特征提取和空间/光谱降维，可以显著减少输入到后续Transformer模块的序列长度，从而间接但有效地降低了整体的计算负担。

​	分层/分级Transformer架构 (Hierarchical Transformers): 采用类似于CNN中特征金字塔的思想，将Transformer应用于不同尺度或不同层次的特征表示之上，逐步构建全局信息，同时在浅层处理较短的序列，在深层处理更抽象但可能更短的序列。

​	特征选择与标记化优化: 设计更为高效的标记化（Tokenization）方法，旨在生成长度更短但信息量更为丰富和浓缩的Token序列，从而在源头上减少Transformer的计算压力。

​	文献[^53]强调Transformer模型能够有效解决传统CNN因感受野受限而难以捕捉全局信息的问题。文献[^12]和[^70]则明确指出Transformer因其自注意力机制的二次计算复杂度而属于计算密集型模型。文献[^54]提到Transformer在处理图像时可能忽略局部细节信息，而CNN在建立长程依赖关系方面效率不高，因此提出了一种U型卷积辅助的Transformer架构（U-shaped Convolution-assisted Transformer）来结合两者的优势。这些研究都清晰地反映了学术界在努力发挥Transformer强大全局建模优势的同时，对其计算效率进行持续优化和改进的重要性。Transformer的全局建模能力是其核心竞争力所在，但计算效率是其在高光谱图像（尤其是面对日益增长的数据量和分辨率）等资源敏感型应用中必须持续攻克和优化的关键技术问题。这一挑战也直接推动了像Mamba这样具有线性计算复杂度的新型序列模型的出现和快速发展，为长序列建模提供了新的解决思路。

 



 

# 6. 基于Mamba (状态空间模型) 的高光谱图像分类方法

​	Mamba作为一种新兴的、基于选择性状态空间模型（Selective State Space Model, SSM 或 S6）的序列建模架构，因其在处理长序列数据时所表现出的卓越计算效率和强大的性能，迅速引起了深度学习研究领域的广泛关注，并已开始在高光谱图像分类这一具有挑战性的任务中得到初步的探索和应用。

 

## 6.1 Mamba模型核心原理及其在长序列建模中的高效性机制


Mamba 模型的核心创新在于其对传统状态空间模型（State Space Model, SSM）的改进，特别是引入了选择性扫描机制（Selective Scan Mechanism, S4/S6），并结合了硬件感知的高效算法设计。经典的状态空间模型通过一个潜在的隐状态 $h(t) \in \mathbb{R}^N$ 来捕捉序列的历史信息，并根据当前输入 $x(t) \in \mathbb{R}^L$ 和上一时刻的状态 $h(t-1)$ 来更新当前状态和产生输出 $y(t) \in \mathbb{R}^L$。其连续时间形式通常可以表示为如下的线性常微分方程（ODE）和输出方程：

$$
h^{\prime}(t) = Ah(t) + Bx(t)
$$
$$
y(t) = Ch(t) + Dx(t)
$$

其中：
* $A \in \mathbb{R}^{N \times N}$ 是状态转移矩阵。
* $B \in \mathbb{R}^{N \times L}$ 是输入矩阵。
* $C \in \mathbb{R}^{L \times N}$ 是输出矩阵。
* $D \in \mathbb{R}^{L \times L}$ 是直通矩阵（通常设为0或可学习）。

然而，传统的 SSM 在直接应用于深度学习和处理长序列时，也面临着梯度消失/爆炸、长程依赖捕捉不足以及计算效率低下等问题。


​	Mamba模型通过以下几个关键方面对SSM进行了改进，从而实现了高效的长序列建模：

​	选择性机制（Selectivity）: Mamba的核心是使其参数（特别是状态转移矩阵$\mathbf{A}$、输入矩阵$\mathbf{B}$和输出矩阵$\mathbf{C}$的关键部分，如时间步长参数$\Delta$）能够依赖于输入数据本身。这意味着模型可以根据输入序列的内容动态地调整其内部状态的演化方式和对信息的关注程度。这种选择性使得Mamba能够有选择地关注或忽略序列中的某些信息片段，并更有效地压缩和传播对当前任务重要的长程依赖信息，从而灵活地建模复杂多变的序列模式。

​	高效的离散化与计算: Mamba采用了一种特定的离散化方法（如零阶保持ZOH或双线性变换）将连续时间的SSM转换为离散时间的表示，并结合了硬件感知的高效算法（例如基于并行扫描的实现、FlashAttention类似的核融合技术）来计算SSM的递归或卷积形式。这使得Mamba在处理长序列时，其计算复杂度相对于序列长度 L 是线性的（O(L)），同时其内存消耗也是线性的[^12]。这与Transformer模型中标准自注意力机制的二次计算复杂度（O(L2)）形成了鲜明的对比。

​	因此，Mamba模型在处理极长的序列数据（例如高光谱图像的光谱维度序列，或者将空谱信息展平后得到的非常长的特征序列）时，具有显著的计算效率优势。文献[^2]明确指出Mamba模型基于选择性状态空间模型（S6），在长序列建模方面展现出巨大的优势。文献[^69]和[^70]也反复强调了Mamba在提供强大的长程序列建模能力的同时，保持了线性的计算复杂度。这种独特的特性为解决Transformer等模型在处理超长序列时面临的计算瓶颈问题提供了一个极具前景的替代方案，对于数据维度通常很高、序列长度极长的高光谱图像而言，其应用潜力尤其具有吸引力。

 

## 6.2 Mamba在高光谱图像分类领域的初步探索与应用范式

​	鉴于Mamba模型在自然语言处理、音频处理等长序列建模任务中取得的突破性进展及其显著的计算效率优势，研究者们迅速开始积极探索将其应用于计算机视觉领域，而高光谱图像分类便是其中一个备受关注且具有挑战性的应用方向[^2]。

​	然而，直接将原始的一维序列Mamba模型应用于高光谱图像分类任务也面临一些不容忽视的挑战。高光谱图像数据具有独特的空谱联合特性（即二维的空间结构与一维的光谱序列紧密耦合，形成三维数据立方体）以及复杂的光谱曲线形态（例如包含细微的诊断性吸收特征、存在显著的光谱变异性等），这与主要处理一维文本序列的NLP场景有所不同[^2]。因此，简单地将高光谱图像数据（例如，将每个像元的光谱曲线视为一个序列，或者将整个图像块展平成一个长序列）直接输入到标准的Mamba模型中，可能无法充分有效地利用其固有的二维空间结构信息和三维空谱联合信息。

​	为了克服这些挑战，并充分发挥Mamba在长程依赖建模方面的优势，研究者们开始着手设计针对高光谱图像数据特性的Mamba变体模型，或者将其与其它成熟的视觉模型（如卷积神经网络CNN、图神经网络GNN）相结合，构建新颖的混合网络架构。这些初步的探索性工作主要集中在2024年和2025年（其中许多成果目前仍以预印本的形式发布在arXiv等平台上），这充分显示出该研究方向正处于快速发展和迭代的初期阶段。

​	在这些初步的探索中，例如，一项发表于2025年arXiv预印本（ID: 2504.15612）的匿名研究提出了HS-Mamba模型，即全场交互多组Mamba框架[^2]。该模型的核心目标在于结合基于像素块的局部特征建模与基于全图像的全局特征感知策略的优点。为此，HS-Mamba设计了包含两个主要并行处理分支的架构。第一个分支是双通道空谱编码器（DCSS-encoder），它采用多组并行的Mamba结构来解耦并分别建模从输入的非重叠图像块中提取的局部空谱特征，通过集成多组Mamba单元、自适应特征拼接策略以及余弦位置编码机制，对从非重叠图像块展平后形成的一维空谱序列进行深度建模。第二个分支是轻量级全局内联注意力（LGI-Att）分支，该分支采用一个轻量级的、基于压缩和扩展思想的注意力模块，用于直接感知未经分割的整个输入图像在空间维度和光谱维度上的全局特征信息。通过有效融合这两个分支提取的局部精细特征和全局上下文特征，HS-Mamba旨在实现高精度、高鲁棒性的高光谱图像分类，其创新点在于这种新颖的“全场交互”分类策略以及DCSS-encoder和LGI-Att模块的特定设计，力求在Mamba框架下更好地适应高光谱数据的特性。

​	另一项代表性工作是Pan, Z.等研究者于2024年在《IEEE Transactions on Geoscience and Remote Sensing》上发表的MambaLG模型[^69]，其全称为Local-to-Global Mamba for Hyperspectral Image Classification。该模型旨在解决标准Mamba模型在有效利用长程依赖关系的同时，可能存在的对局部细节信息捕捉不足的问题。为此，MambaLG提出了一种精心设计的“局部到全局”双分支特征学习策略。其中，局部和全局空间建模模块（SpaM）通过序贯地提取和整合图像的局部空间信息与全局空间信息，以期在捕获全局空间语义上下文的同时，有效保留和利用局部的二维空间结构特征。而短程和长程光谱动态感知模块（SpeM）则专注于光谱维度的信息处理，它首先利用局部光谱特征提取单元，然后结合光谱分组策略和一种称为光谱动态相关聚类（SDCC）的模块，并巧妙地融合Mamba模型在长程序列建模方面的优势，从而实现对光谱数据中短程和长程依赖特征的精确建模与感知。此外，MambaLG还引入了门控注意力单元来增强特征选择性，并设计了更为高效、更具可解释性的空谱特征融合方法。该模型在多个包含城市和农业等复杂场景的公开高光谱图像数据集上进行了实验验证，并取得了超越当时其他主流SOTA算法的分类性能。

​	此外，Ahmad, M.等研究者在一篇2025年的arXiv预印本 (ID: 2502.06427) 中提出了GraphMamba模型，全称为Hybrid State-Space and GRU-based Graph Tokenization Mamba for Hyperspectral Image Classification [^12]。该模型提出了一种新颖的混合状态空间模型（以Mamba为核心）和门控循环单元（GRU）的图标记化Mamba架构。其核心目标是通过有机地结合谱空特征标记生成、基于图结构的标记优先化以及交叉注意力机制，来有效地捕获高光谱数据中复杂且相互交织的线性和非线性空谱动态特征。GraphMamba的创新点体现在多个方面：它采用一个双卷积框架（例如，1x1卷积处理光谱信息，3x3卷积处理空间信息）进行高效的初始特征提取和初步标记化；设计了一个新颖的图标记化模块，通过可学习的评分机制动态优先化不同的特征标记，并将这些标记建模为图节点，节点间的边由相似性计算的邻接矩阵定义；采用交叉注意力层促进谱标记和空标记间的动态交互；通过特征融合层整合图模型和注意力机制的输出；并构建了一个混合SSM层，将GRU集成到状态空间模型中以捕获复杂的谱空依赖。该模型在多个常用的基准高光谱图像数据集上（如Pavia University, Pavia Center, Salinas, University of Houston, Hyperion-based Dataset）取得了SOTA级别的分类性能，并且其模型参数量显著少于许多对比的先进模型。

​	此外，文献[^12]在其相关工作部分还提及了其他一些更早期的、将Mamba模型应用于高光谱图像分类的探索性研究工作，例如SS-Mamba (Spectral-Spatial Mamba), MambaHSI, MiM (Mamba-in-Mamba), HyperMamba, WaveMamba, 以及Spatial-Spectral Morphological Mamba等。这些多样化的研究共同构成了Mamba模型在高光谱图像分类领域应用的初步版图和技术积累。它们的核心探索方向是如何将Mamba模型强大的序列建模能力与高光谱数据固有的、复杂的三维空谱特性进行有效的结合与适配。当前Mamba在高光谱图像分类领域的研究，在很大程度上是在借鉴和适配先前Transformer模型在HSI应用中所取得的成功经验（例如，广泛采用CNN-Transformer混合架构的思想），但其核心动机在于试图用Mamba替换Transformer，以期在保持甚至提升分类性能的同时，获得在计算效率（特别是处理长序列时的线性复杂度）上的显著提升。

 

## 6.3 Mamba与CNN、GNN等主流模型的深度结合策略与巨大潜力

​	正如Transformer模型在高光谱图像分类领域的发展路径所揭示的那样，单一类型的深度学习模型（无论是CNN、RNN、GNN、Transformer还是Mamba）往往难以完美地应对高光谱数据所呈现的多方面复杂挑战（如高维性、小样本、光谱变异性、空谱信息耦合等）。因此，将Mamba模型与卷积神经网络（CNN）、图神经网络（GNN）等已经相对成熟且在高光谱领域被证明行之有效的深度学习架构进行有机的结合，构建新颖的混合模型（Hybrid Models），被认为是充分发挥Mamba在长序列建模和计算效率方面潜力、进一步提升高光谱图像分类综合性能的关键策略和重要研究方向。

1. Mamba与CNN的结合 (Mamba-CNN Hybrid Architectures):

​	核心策略: CNN因其在提取图像局部特征（如纹理、边缘、光谱形状片段等）方面的强大能力和固有优势（如局部连接、权值共享、平移不变性），可以作为Mamba模型的前端特征提取器或特征增强模块。在这种混合架构中，通常首先利用1D、2D或更常见的3D CNN从原始高光谱数据（或其邻域子块）中提取浅层的、局部的空谱联合特征。这些由CNN输出的特征图（Feature Maps）随后被进行有效的序列化处理（例如，可以将空间特征图展平成一维序列，或者沿着光谱维度将每个空间位置的特征向量排列成序列），然后将这些表示局部空谱信息的序列输入到后续的Mamba模型中，由Mamba负责进行长程依赖关系的建模和全局上下文信息的深度整合。这种设计思路与先前成功的CNN-Transformer混合模型的理念高度相似，但其核心期望在于通过Mamba替换Transformer来获得在处理长序列时更高的计算效率和更低的内存消耗。

​	巨大潜力: 这种Mamba-CNN的结合方式能够充分利用CNN在识别图像局部模式方面的优势，为Mamba提供更具信息量、更紧凑、噪声更少、抽象层次更高的输入序列。这不仅可能提升Mamba模型在高光谱分类任务上的性能表现，也可能进一步降低其直接处理原始高维、高冗余度高光谱数据的计算负担和学习难度。

2. Mamba与GNN的结合 (Mamba-GNN Hybrid Architectures):

​	核心策略: GNN擅长于建模数据样本之间复杂的、非欧几里得的结构关系。在Mamba-GNN混合模型的设计中，GNN可以用于构建和学习高光谱图像中像元（或超像元）之间的关系图，从而显式地捕捉它们之间的空间邻近性、光谱相似性、甚至是更复杂的语义关联。Mamba模型则可以被用于处理从这个图结构中提取出来的各种序列信息，例如，可以处理图中节点的特征序列（按某种顺序排列）、图上随机游走生成的路径序列、或者整个图的某种线性化或序列化表示。前述的GraphMamba[^12]模型就是一个典型的Mamba-GNN结合范例，它首先通过图标记化模块将由CNN提取的空谱特征组织成图结构，并利用交叉注意力机制来促进不同标记（节点）之间的信息交互，最终将融合后的信息输入到包含GRU单元的混合状态空间模型（以Mamba为核心）中进行分类决策。

​	巨大潜力: 这种Mamba-GNN的结合有望将GNN在关系推理、结构化信息表示和利用非局部上下文方面的优势，与Mamba在高效长序列建模方面的优势进行深度融合，从而能够更全面、更深刻地理解高光谱数据中蕴含的复杂模式和内在规律。例如，GNN可以帮助识别出那些在空间上可能相距较远但在光谱特性上高度相似的区域（即捕捉非局部相似性），Mamba则可以在这些由GNN识别出来的相关节点所构成的序列上进行更深层次的特征学习和依赖关系建模。

3. Mamba与注意力机制的结合 (Mamba-Attention Hybrid Architectures):

​	核心策略: Mamba模型本身就内含一种基于输入的选择性机制（Selectivity），这在某种程度上可以被视为一种内在的、隐式的注意力形式，因为它允许模型根据输入动态地调整其状态转换和信息处理方式。除此之外，还可以将外部的、更显式的注意力模块（例如空间注意力机制、通道注意力机制、或者类似于Transformer中的自注意力机制及其变体）与Mamba模型进行灵活的结合使用。例如，在HS-Mamba[^2]模型中，除了其核心的Mamba处理模块外，还专门设计了一个轻量级的全局内联注意力（LGI-Att）分支，用于增强模型对整个输入图像全局特征的感知能力。

​	巨大潜力: 外部引入的注意力机制可以帮助Mamba模型在进行序列建模之前（例如对输入序列进行加权或选择）或之后（例如对Mamba输出的特征进行增强或融合），更好地聚焦于对当前分类任务更重要、信息量更丰富的空间区域、光谱波段或特征通道，从而提升最终学习到的特征的判别性和模型的整体鲁棒性。

​	这些多样化的混合策略其核心思想在于实现不同模型之间的“优势互补”和“协同增强”。Mamba以其高效的长程序列建模能力，可以有效弥补传统CNN在全局上下文感知方面的固有不足，或者为GNN处理的图节点序列（或其特征）提供一种新颖且高效的分析工具。反过来，CNN强大的局部特征提取能力和GNN灵活的关系建模与结构信息表示能力，也为Mamba模型更有效地处理复杂的高光谱空谱数据提供了有力的支持和高质量的输入。这种“各司其职、协同作战”的模块化、层次化混合模型设计理念，是充分发挥Mamba模型在高光谱图像分类任务中巨大潜力的关键所在，有望在未来催生出性能更优、效率更高、鲁棒性更强的新一代高光谱图像智能分类解决方案。

 

## 6.4 Mamba模型在高光谱图像分类中的发展前景展望

​	Mamba模型作为序列建模领域的一项重要技术突破，凭借其在处理长序列数据时所展现出的线性计算复杂度（相对于序列长度）和强大的长程依赖建模能力，被学术界和工业界广泛认为是Transformer模型在某些特定应用场景下的有力竞争者或潜在替代者。尤其是在处理大规模高光谱图像数据（其光谱维和空间维展开后都可能形成极长的序列）或需要在资源受限的计算平台（例如星上智能处理系统、无人机载实时分析模块、边缘计算设备等）上部署深度学习模型时，Mamba模型的计算效率优势使其具有尤为突出的应用潜力[^12]。

​	尽管Mamba模型在高光谱图像分类领域的应用目前尚处于相对初步的探索阶段，但已初步展现出其在解决现有主流模型（特别是基于标准Transformer的模型）所面临的痛点问题（尤其是高计算成本和内存消耗）方面的巨大潜力。其未来的发展前景广阔，可能的研究方向和重点包括：

​	针对高光谱数据特性的Mamba架构创新: 目前应用于高光谱图像的Mamba模型大多是直接借鉴或简单修改自最初为自然语言处理（NLP）等一维序列任务设计的原始Mammable架构。未来需要投入更多研究力量，设计出能够更深度契合高光谱数据独特的三维（空间-空间-光谱）结构、光谱曲线的连续性与局部诊断性特征、空间上下文的相关性以及普遍存在的混合像元等内在特性的新型Mamba变体或专用架构。这可能涉及到对Mamba核心的选择性扫描机制（S4/S6）、状态空间表示方式、以及输入输出接口（如Tokenization和特征融合）的专门优化和改进。

​	高效的空谱信息标记化（Tokenization）与多模态融合机制: 如何将高光谱图像中复杂的空谱联合信息有效地转换为Mamba模型能够高效处理的序列形式（即Tokenization过程），并设计出更为精巧、信息损失更少的Mamba与CNN、GNN等其他模态特征提取器（如提取空间特征的CNN、提取光谱特征的1D-CNN或RNN、提取图结构关系的GNN）的深度融合机制，将是持续的研究热点。这需要在保证信息保留的完整性和序列处理的效率之间进行仔细的权衡和创新的设计。

​	向其他高光谱遥感相关任务的扩展应用: Mamba模型的成功应用不应仅仅局限于图像分类任务。其在高效长序列建模方面的核心优势，使其在高光谱遥感相关的其他重要任务中也具有广阔的应用潜力。例如，高光谱变化检测（文献[^70]已提及GASSM模型将Mamba用于此任务并取得良好效果）、高光谱目标检测与识别、高光谱图像去噪与复原、高光谱解混（端元提取与丰度反演）、以及多时相高光谱图像序列分析（如物候监测、土地利用动态变化跟踪）等。

​	大规模数据集验证、模型鲁棒性与可解释性研究: 目前Mamba模型在高光谱图像分类中的研究大多基于数量有限、场景相对单一的公开基准数据集。未来需要在更大规模、更多样化、更接近真实应用场景的高光谱数据集上对其性能、鲁棒性（如对噪声、光照变化、传感器差异的适应性）和可扩展性进行全面深入的验证。同时，对Mamba模型内部工作机制（例如，其选择性扫描机制是如何影响特征学习和长程依赖捕捉的）的可解释性研究也亟待加强，以增强我们对模型行为的理解和信任。

​	与新兴人工智能技术的深度结合: 将Mamba模型与自监督学习（Self-supervised Learning）、半监督学习（Semi-supervised Learning）、无监督学习、物理信息神经网络（Physics-Informed Neural Networks, PINN）、知识蒸馏（Knowledge Distillation）、联邦学习（Federated Learning）等新兴的人工智能技术和范式进行深度结合，有望进一步提升其在小样本学习、模型轻量化、融合领域先验知识、保护数据隐私等方面的能力，拓展其应用边界。

​	正如文献[^69]的作者所指出的，基于Mamba的方法为高光谱图像分类开辟了一个充满希望的新窗口。文献[^12]则强调其提出的GraphMamba模型为复杂的高光谱图像分类任务提供了一个具有良好可扩展性和鲁棒性的解决方案。文献[^70]也认为Mamba为高光谱变化检测任务提供了一个极具前景的替代方案。这些积极的评价和初步的研究成果都预示着Mamba模型代表了序列建模领域的一个重要发展方向，其在高光谱图像处理乃至整个遥感与地球观测领域的应用潜力值得持续关注和深入挖掘。Mamba模型的出现和快速发展，是深度学习领域对“效率与性能并重”这一核心目标的持续追求的生动体现。它不仅有可能成为未来高光谱图像分类领域的新一代主力模型之一，也可能在其他众多需要高效处理长序列数据的遥感信息智能处理任务中发挥关键作用，代表了从早期“不计成本追求极致性能”到当前“在高性能和高效率之间寻求更优平衡与协同发展”的重要研究趋势。

 

 

# 7. 各类方法性能比较与讨论

​	对高光谱图像分类中各类深度学习与机器学习方法进行性能比较和深入讨论，有助于理解不同模型的优势、局限以及适用场景，从而为未来的研究和应用提供指导。

 

## 7.1 常用基准数据集与评估指标

​	为了客观、公正地评估和比较不同HSI分类算法的性能，研究界广泛采用一系列公开的基准数据集和标准化的评估指标。

### 7.1.1 常用基准数据集:

#### 7.1.1.1 Botswana

​	The NASA EO-1 satellite acquired a sequence of data over the Okavango Delta, Botswana in 2001-2004. The Hyperion sensor on EO-1 acquires data at 30 m pixel resolution over a 7.7 km strip in 242 bands covering the 400-2500 nm portion of the spectrum in 10 nm windows. Preprocessing of the data was performed by the UT Center for Space Research to mitigate the effects of bad detectors, inter-detector miscalibration, and intermittent anomalies. Uncalibrated and noisy bands that cover water absorption features were removed, and the remaining 145 bands were included as candidate features: [10-55, 82-97, 102-119, 134-164, 187-220]. The data analyzed in this study, acquired May 31, 2001, consist of observations from 14 identified classes representing the land cover types in seasonal swamps, occasional swamps, and drier woodlands located in the dist al portion of the Delta.

![](./assets/image-20241219182110164.png)

Groundtruth classes for the Botswana scene and their respective samples number



|  #   |        Class         | Samples |
| :--: | :------------------: | :-----: |
|  1   |        Water         |   270   |
|  2   |     Hippo grass      |   101   |
|  3   | Floodplain grasses 1 |   251   |
|  4   | Floodplain grasses 2 |   215   |
|  5   |        Reeds         |   269   |
|  6   |       Riparian       |   269   |
|  7   |       Firescar       |   259   |
|  8   |   Island interior    |   203   |
|  9   |   Acacia woodlands   |   314   |
|  10  |  Acacia shrublands   |   248   |
|  11  |  Acacia grasslands   |   305   |
|  12  |     Short mopane     |   181   |
|  13  |     Mixed mopane     |   268   |
|  14  |    Exposed soils     |   95    |



[^Botswana]: https://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Botswana



#### 7.1.1.2 WHU-Hi-LongKou dataset

​	The WHU-Hi dataset preprocessing included radiometric calibration and geometric correction, which were undertaken in the HyperSpec software provided by the instrument manufacturer. For the radiometric calibration, the raw digital number values were converted into radiance values by the laboratory calibration parameters of the sensor.[^WHU-Hi]



[^WHU-Hi]: https://rsidea.whu.edu.cn/resource_WHUHi_sharing.htm

  The WHU-Hi-LongKou dataset was acquired from 13:49 to 14:37 on July 17, 2018, in Longkou Town, Hubei province, China, with an 8-mm focal length Headwall Nano-Hyperspec imaging sensor equipped on a DJI Matrice 600 Pro (DJI M600 Pro) UAV platform. During the data collection, the weather was clear and cloudless, the temperature was about 36°C, and the relative air humidity was about 65%. The study area is a simple agricultural scene, which contains six crop species: corn, cotton, sesame, broad-leaf soybean, narrow-leaf soybean, and rice. The UAV flew at an altitude of 500 m, the size of the imagery is 550 × 400 pixels, there are 270 bands from 400 to 1000 nm, and the spatial resolution of the UAV-borne hyperspectral imagery is about 0.463 m. An overview of this dataset is provided in Fig. 1 and Table. 1.

The WHU-Hi-LongKou dataset was acquired from 13:49 to 14:37 on July 17, 2018, in Longkou Town, Hubei province, China, with an 8-mm focal length Headwall Nano-Hyperspec imaging sensor equipped on a DJI Matrice 600 Pro (DJI M600 Pro) UAV platform. The DJI M600 Pro UAV possesses a 6-kg maximum payload capacity, it has a flight time of approximately 30 min, and is equipped with GPS/IMU modules for centimeter-level navigation. During the data collection, the weather was clear and cloudless, the temperature was about 36 °C, and the relative air humidity was about 65%. The study area contains six crop species: corn, cotton, sesame, broad-leaf soybean, narrow-leaf soybean, and rice. The UAV flew at an altitude of 500 m, the size of the imagery is 550 × 400 pixels, there are 270 bands from 400 to 1000 nm, and the spatial resolution of the UAV-borne hyperspectral imagery is about 0.463 m. 



![image-20241219182401295](./assets/image-20241219182401295.png)


|  #   |        Class        | Samples |
| :--: | :-----------------: | :-----: |
|  1   |        Corn         |  34511  |
|  2   |       Cotton        |  8374   |
|  3   |       Sesame        |  3031   |
|  4   | Broad-leaf soybean  |  63212  |
|  5   | Narrow-leaf soybean |  4151   |
|  6   |        Rice         |  11854  |
|  7   |        Water        |  67056  |
|  8   |  Roads and houses   |  7124   |
|  9   |     Mixed weed      |  5229   |



#### 7.1.1.3 WHU-Hi-HanChuan dataset

The WHU-Hi dataset preprocessing included radiometric calibration and geometric correction, which were undertaken in the HyperSpec software provided by the instrument manufacturer. For the radiometric calibration, the raw digital number values were converted into radiance values by the laboratory calibration parameters of the sensor.[^WHU-Hi]

  The WHU-Hi-HanChuan dataset was acquired from 17:57 to 18:46 on June 17, 2016, in Hanchuan, Hubei province, China, with an 17-mm focal length Headwall Nano-Hyperspec imaging sensor equipped on a Leica Aibot X6 UAV V1 platform. During the data collection, the weather was clear and cloudless, the temperature was about 30°C, and the relative air humidity was about 70%. The study area is a rural-urban fringe zone with buildings, water, and cultivated land, which contains seven crop species: strawberry, cowpea, soybean, sorghum, water spinach, watermelon, and greens. The UAV flew at an altitude of 250 m, the size of the imagery is 1217 × 303 pixels, there are 274 bands from 400 to 1000 nm, and the spatial resolution of the UAV-borne hyperspectral imagery is about 0.109 m. Notably, since the WHU-Hi-HanChuan dataset was acquired during the afternoon when the solar elevation angle was low, there are many shadow-covered areas in the image. An overview of this dataset is given in Fig. 2 and Table. 2.



![image-20241219182136557](./assets/image-20241219182136557.png)

| **No.** | **Class name** | **Samples** |
| ------- | -------------- | ----------- |
| 1       | Strawberry     | 44735       |
| 2       | Cowpea         | 22753       |
| 3       | Soybean        | 10287       |
| 4       | Sorghum        | 5353        |
| 5       | Water spinach  | 1200        |
| 6       | Watermelon     | 4533        |
| 7       | Greens         | 5903        |
| 8       | Trees          | 17978       |
| 9       | Grass          | 9469        |
| 10      | Red roof       | 10516       |
| 11      | Gray roof      | 16911       |
| 12      | Plastic        | 3679        |
| 13      | Bare soil      | 9116        |
| 14      | Road           | 18560       |
| 15      | Bright object  | 1136        |
| 16      | Water          | 75401       |







#### 7.1.1.4 WHU-Hi-HongHu dataset

The WHU-Hi dataset preprocessing included radiometric calibration and geometric correction, which were undertaken in the HyperSpec software provided by the instrument manufacturer. For the radiometric calibration, the raw digital number values were converted into radiance values by the laboratory calibration parameters of the sensor.[^WHU-Hi]

  The WHU-Hi-HongHu dataset was acquired from 16:23 to 17:37 on November 20, 2017, in Honghu City, Hubei province, China, with a 17-mm focal length Headwall Nano-Hyperspec imaging sensor equipped on a DJI Matrice 600 Pro UAV platform. During the data collection, the weather was cloudy, the temperature was about 8°C, and the relative air humidity was about 55%. The experimental area is a complex agricultural scene with many classes of crops, and different cultivars of the same crop are also planted in the region, including Chinese cabbage and cabbage, and Brassica chinensis and small Brassica chinensis. Notably, the region is planted with different cultivars of the same crop type; for example, Chinese cabbage/cabbage and brassica chinensis/small brassica chinensis. The UAV flew at an altitude of 100 m, the size of the imagery is 940 × 475 pixels, there are 270 bands from 400 to 1000 nm, and the spatial resolution of the UAV-borne hyperspectral imagery is about 0.043 m. An overview of this dataset is provided in Fig. 3 and Table. 3.



![image-20241219182202603](./assets/image-20241219182202603.png)

| No.  |        Class name        | Samples |
| :--: | :----------------------: | :-----: |
|  1   |         Red roof         |  14041  |
|  2   |           Road           |  3512   |
|  3   |        Bare soil         |  21821  |
|  4   |          Cotton          | 163285  |
|  5   |     Cotton firewood      |  6218   |
|  6   |           Rape           |  44557  |
|  7   |     Chinese cabbage      |  24103  |
|  8   |         Pakchoi          |  4054   |
|  9   |         Cabbage          |  10819  |
|  10  |      Tuber mustard       |  12394  |
|  11  |  Brassica parachinensis  |  11015  |
|  12  |    Brassica chinensis    |  8954   |
|  13  | Small Brassica chinensis |  22507  |
|  14  |      Lactuca sativa      |  7356   |
|  15  |         Celtuce          |  1002   |
|  16  |   Film covered lettuce   |  7262   |
|  17  |     Romaine lettuce      |  3010   |
|  18  |          Carrot          |  3217   |
|  19  |       White radish       |  8712   |
|  20  |      Garlic sprout       |  3486   |
|  21  |        Broad bean        |  1328   |
|  22  |           Tree           |  4040   |



#### 7.1.1.5 Houston2013

Houston数据是为 2013 IEEE Geoscience and Remote Sensing Society (GRSS) data fusion contest而分发的。该场景是 2012 年由机载传感器在休斯顿大学校园和邻近城区上空拍摄的。数据大小为 349 × 1905 像素，空间分辨率为 2.5 m。该 HSI 由 144 个光谱带组成，波长范围从 0.38 到 1.05 μm，包括 15 个类。图 13 分别显示了休斯顿数据集和相应地面参考数据的假彩色合成图。

Houston 2013 Dataset:This scene was acquired by the ITRES CASI-1500 sensor over the campus of the University of Houston and its neighboring rural areas, and it was used in the 2013 GRSS Data Fusion Contest.The data comprises 144 spectral bands in the 380–1050 nm region and has dimensions of 349 × 1905 pixels with a spatial resolution of 2.5 m.



The Houston data were distributed for the 2013 IEEE Geoscience and Remote Sensing Society (GRSS) data fusion contest. This scene was captured in 2012 by an airborne sensor over the area of University of Houston campus and the neighboring urban area. The size of the data is 349 × 1905 pixels with a spatial resolution of 2.5 m. This HSI consists of 144 spectral bands with wavelength ranging from 0.38 to 1.05 μm and includes 15 classes.[^Houston2013]

[^Houston2013]:https://hyperspectral.ee.uh.edu/?page_id=459





![image-20241219182305522](../../.././assets/image-20241219182305522.png)

| No.  |   Class name    | Samples |
| :--: | :-------------: | :-----: |
|  1   |  Healthy grass  |  1251   |
|  2   | Stressed grass  |  1254   |
|  3   | Synthetic grass |   697   |
|  4   |      Tree       |  1244   |
|  5   |      Soil       |  1242   |
|  6   |      Water      |   325   |
|  7   |   Residential   |  1268   |
|  8   |   Commercial    |  1244   |
|  9   |      Road       |  1252   |
|  10  |     Highway     |  1227   |
|  11  |     Railway     |  1235   |
|  12  |  Parking lot 1  |  1233   |
|  13  |  Parking lot 2  |   469   |
|  14  |  Tennis court   |   428   |
|  15  |  Running track  |   660   |





#### 7.1.1.6 Houston2018

2018 IEEE GRSS Data Fusion Challenge – Fusion of Multispectral LiDAR and Hyperspectral Data [^2018 IEEE GRSS]

[^2018 IEEE GRSS]:https://machinelearning.ee.uh.edu/2018-ieee-grss-data-fusion-challenge-fusion-of-multispectral-lidar-and-hyperspectral-data/



Houston 2018 Dataset:The Houston 2018 dataset was acquired by the Hyperspectral Image Analysis Group and the NSF-funded Airborne Laser Mapping Center (NCALM) at the University of Houston, covering the university campus and surrounding areas.The data size is 601 x 2384 pixels, with 504,856 pixel samples labeled for training and testing purposes in hyperspectral image classification.The wavelength range of the image data is from 0.38 to 1.05 μm, with a total of 50 bands.[^Houston2018]

[^Houston2018]:http://hyperspectral.ee.uh.edu/QZ23es1aMPH/2018IEEE/phase2.zip



![image-20241219182244057](./assets/image-20241219182244057.png)

| No.  |        Class name         | Samples |
| :--: | :-----------------------: | :-----: |
|  0   |         Undefined         | 927928  |
|  1   |       Healthy grass       |  9799   |
|  2   |      Stressed grass       |  32502  |
|  3   |      Artificial turf      |   684   |
|  4   |      Evergreen trees      |  13595  |
|  5   |      Deciduous trees      |  5021   |
|  6   |        Bare earth         |  4516   |
|  7   |           Water           |   266   |
|  8   |   Residential buildings   |  39772  |
|  9   | Non-residential buildings | 223752  |
|  10  |           Roads           |  45866  |
|  11  |         Sidewalks         |  34029  |
|  12  |        Crosswalks         |  1518   |
|  13  |    Major thoroughfares    |  46348  |
|  14  |         Highways          |  9865   |
|  15  |         Railways          |  6937   |
|  16  |    Paved parking lots     |  11500  |
|  17  |   Unpaved parking lots    |   146   |
|  18  |           Cars            |  6547   |
|  19  |          Trains           |  5369   |
|  20  |       Stadium seats       |  6824   |





#### 7.1.1.7 Indian Pines

This scene was gathered by AVIRIS sensor over the Indian Pines test site in North-western Indiana and consists of 145\times145 pixels and 224 spectral reflectance bands in the wavelength range 0.4–2.5 10^(-6) meters. This scene is a subset of a larger one. The Indian Pines scene contains two-thirds agriculture, and one-third forest or other natural perennial vegetation. There are two major dual lane highways, a rail line, as well as some low density housing, other built structures, and smaller roads. Since the scene is taken in June some of the crops present, corn, soybeans, are in early stages of growth with less than 5% coverage. The ground truth available is designated into sixteen classes and is not all mutually exclusive. We have also reduced the number of bands to 200 by removing bands covering the region of water absorption: [104-108], [150-163], 220. [^Indian Pines]



[^Indian Pines]:https://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Indian_Pines



![image-20241219182328109](./assets/image-20241219182328109.png)



Groundtruth classes for the Indian Pines scene and their respective samples number

|  #   |            Class             | Samples |
| :--: | :--------------------------: | :-----: |
|  1   |           Alfalfa            |   46    |
|  2   |         Corn-notill          |  1428   |
|  3   |         Corn-mintill         |   830   |
|  4   |             Corn             |   237   |
|  5   |        Grass-pasture         |   483   |
|  6   |         Grass-trees          |   730   |
|  7   |     Grass-pasture-mowed      |   28    |
|  8   |        Hay-windrowed         |   478   |
|  9   |             Oats             |   20    |
|  10  |        Soybean-notill        |   972   |
|  11  |       Soybean-mintill        |  2455   |
|  12  |        Soybean-clean         |   593   |
|  13  |            Wheat             |   205   |
|  14  |            Woods             |  1265   |
|  15  | Buildings-Grass-Trees-Drives |   386   |
|  16  |      Stone-Steel-Towers      |   93    |





#### 7.1.1.8 Kennedy Space Center (KSC)

The NASA AVIRIS (Airborne Visible/Infrared Imaging Spectrometer) instrument acquired data over the Kennedy Space Center (KSC), Florida, on March 23, 1996. AVIRIS acquires data in 224 bands of 10 nm width with center wavelengths from 400 - 2500 nm. The KSC data, acquired from an altitude of approximately 20 km, have a spatial resolution of 18 m. After removing water absorption and low SNR bands, 176 bands were used for the analysis. Training data were selected using land cover maps derived from color infrared photography provided by the Kennedy Space Center and Landsat Thematic Mapper (TM) imagery. The vegetation classification scheme was developed by KSC personnel in an effort to define functional types that are discernable at the spatial resolution of Landsat and these AVIRIS data. Discrimination of land cover for this environment is difficult due to the similarity of spectral signatures for certain vegetation types. For classification purposes, 13 classes representing the various land cover types that occur in this environment were defined for the site.[^Kennedy Space Center]



[^Kennedy Space Center]:https://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Indian_Pines





![image-20241219182342619](./assets/image-20241219182342619.png)



Groundtruth classes for the Salinas scene and their respective samples number

|  #   |           Class           | Samples |
| :--: | :-----------------------: | :-----: |
|  1   |   Brocoli_green_weeds_1   |  2009   |
|  2   |   Brocoli_green_weeds_2   |  3726   |
|  3   |          Fallow           |  1976   |
|  4   |     Fallow_rough_plow     |  1394   |
|  5   |       Fallow_smooth       |  2678   |
|  6   |          Stubble          |  3959   |
|  7   |          Celery           |  3579   |
|  8   |     Grapes_untrained      |  11271  |
|  9   |   Soil_vinyard_develop    |  6203   |
|  10  | Corn_senesced_green_weeds |  3278   |
|  11  |    Lettuce_romaine_4wk    |  1068   |
|  12  |    Lettuce_romaine_5wk    |  1927   |
|  13  |    Lettuce_romaine_6wk    |   916   |
|  14  |    Lettuce_romaine_7wk    |  1070   |
|  15  |     Vinyard_untrained     |  7268   |
|  16  | Vinyard_vertical_trellis  |  1807   |







#### 7.1.1.9 Pavia Center scence

These are two scenes acquired by the [ROSIS sensor](http://www.opairs.aero/rosis_en.html) during a flight campaign over Pavia, nothern Italy. The number of spectral bands is 102 for Pavia Centre and 103 for Pavia University. Pavia Centre is a 1096*1096 pixels image, and Pavia University is 610*610 pixels, but some of the samples in both images contain no information and have to be discarded before the analysis. The geometric resolution is 1.3 meters. Both image groundtruths differenciate 9 classes each. It can be seen the discarded samples in the figures as abroad black strips.[^Pavia Center scence]

Pavia scenes were provided by [Prof. Paolo Gamba](http://tlclab.unipv.it/sito_tlc/people.do?id=pgamba) from the [Telecommunications and Remote Sensing Laboratory](http://tlclab.unipv.it/), [Pavia university](http://www.unipv.eu/) (Italy).






![image-20241219182418194](../../.././assets/image-20241219182418194.png)




Groundtruth classes for the Pavia centre scene and their respective samples number

|  #   |        Class         | Samples |
| :--: | :------------------: | :-----: |
|  1   |        Water         |   824   |
|  2   |        Trees         |   820   |
|  3   |       Asphalt        |   816   |
|  4   | Self-Blocking Bricks |   808   |
|  5   |       Bitumen        |   808   |
|  6   |        Tiles         |  1260   |
|  7   |       Shadows        |   476   |
|  8   |       Meadows        |   824   |
|  9   |      Bare Soil       |   820   |






[^Pavia Center scence]: https://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_scene



#### 7.1.1.10 Pavia University scene

Pavia University 捕捉了意大利Pavia University 周围的城市地区，由 ROSIS-03 传感器于 2001 年在意大利北部收集。该场景的大小为 610 × 340 × 115，空间分辨率为每像素 1.3 米，光谱覆盖范围为 0.43 至 0.86 微米。该图像包括 9 个感兴趣的类别，在去除 12 个非常嘈杂的波段后有 103 个光谱波段。

The University of Pavia data set, which captures an urban area surrounding the University of Pavia, Italy, was collected by the ROSIS-03 sensor in Northern Italy in 2001. This scene is of size 610 × 340 × 115 with a spatial resolution of 1.3 m per pixel and spectral coverage ranging from 0.43 to 0.86 μm. This image includes 9 classes of interest and has 103 spectral bands after removing 12 very noisy bands.

Pavia University: The image was captured by the Reflective Optics System Imaging Spectrometer (ROSIS) sensor over Pavia University, consisting 103 bands with a spatial size of 610 × 340. The dataset contains 42776 labeled pixels of nine classes. The related information, such as original image visualization, ground truth map, and the dataset split configuration, is shown in Fig.



These are two scenes acquired by the [ROSIS sensor](http://www.opairs.aero/rosis_en.html) during a flight campaign over Pavia, nothern Italy. The number of spectral bands is 102 for Pavia Centre and 103 for Pavia University. Pavia Centre is a 1096*1096 pixels image, and Pavia University is 610*610 pixels, but some of the samples in both images contain no information and have to be discarded before the analysis. The geometric resolution is 1.3 meters. Both image groundtruths differenciate 9 classes each. It can be seen the discarded samples in the figures as abroad black strips.

Pavia scenes were provided by [Prof. Paolo Gamba](http://tlclab.unipv.it/sito_tlc/people.do?id=pgamba) from the [Telecommunications and Remote Sensing Laboratory](http://tlclab.unipv.it/), [Pavia university](http://www.unipv.eu/) (Italy).


![image-20241219182433020](../../.././assets/image-20241219182433020.png)


Groundtruth classes for the Pavia University scene and their respective samples number

|  #   |        Class         | Samples |
| :--: | :------------------: | :-----: |
|  1   |       Asphalt        |  6631   |
|  2   |       Meadows        |  18649  |
|  3   |        Gravel        |  2099   |
|  4   |        Trees         |  3064   |
|  5   | Painted metal sheets |  1345   |
|  6   |      Bare Soil       |  5029   |
|  7   |       Bitumen        |  1330   |
|  8   | Self-Blocking Bricks |  3682   |
|  9   |       Shadows        |   947   |




[^Pavia University scene]: https://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_University_scene



#### 7.1.1.11 Salinas

Salinas [Link](https://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Salinas) [^Salinas].

Salinas由加利福尼亚州萨利纳斯谷上空的机载可见光/红外成像光谱仪传感器捕获。该图像包含 512 × 217 像素，空间分辨率为 3.7 米，在去除 20 个水吸收波段后有 204 个波段。可用的地面参考图涵盖 16 个感兴趣的类别。萨利纳斯图像的假彩色合成图和相应的地面参考数据如图 15 所示。

The Salinas data set was captured by the Airborne Visible/Infrared Imaging Spectrometer sensor over Salinas Valley, California. This image comprises of 512 × 217 pixels with a spatial resolution of 3.7 m and 204 bands after removing 20 water absorption bands. The available ground reference map covers 16 classes of interest. 

This scene was collected by the 224-band [AVIRIS sensor](http://aviris.jpl.nasa.gov/) over Salinas Valley, California, and is characterized by high spatial resolution (3.7-meter pixels). The area covered comprises 512 lines by 217 samples. As with Indian Pines scene, we discarded the 20 water absorption bands, in this case bands: [108-112], [154-167], 224. This image was available only as at-sensor radiance data. It includes vegetables, bare soils, and vineyard fields. Salinas groundtruth contains 16 classes.






![image-20241219182452108](./assets/image-20241219182452108.png)




Groundtruth classes for the Salinas scene and their respective samples number

|  #   |           Class           | Samples |
| :--: | :-----------------------: | :-----: |
|  1   |   Brocoli_green_weeds_1   |  2009   |
|  2   |   Brocoli_green_weeds_2   |  3726   |
|  3   |          Fallow           |  1976   |
|  4   |     Fallow_rough_plow     |  1394   |
|  5   |       Fallow_smooth       |  2678   |
|  6   |          Stubble          |  3959   |
|  7   |          Celery           |  3579   |
|  8   |     Grapes_untrained      |  11271  |
|  9   |   Soil_vinyard_develop    |  6203   |
|  10  | Corn_senesced_green_weeds |  3278   |
|  11  |    Lettuce_romaine_4wk    |  1068   |
|  12  |    Lettuce_romaine_5wk    |  1927   |
|  13  |    Lettuce_romaine_6wk    |   916   |
|  14  |    Lettuce_romaine_7wk    |  1070   |
|  15  |     Vinyard_untrained     |  7268   |
|  16  | Vinyard_vertical_trellis  |  1807   |






[^Salinas]: https://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Salinas



#### 7.1.1.12 SalinasA

Salinas-A [Link](https://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Salinas-A_scene) [^Salinas-A].

An small subscene of Salinas image, denoted Salinas-A, is usually used too. It comprises 86*83 pixels located within the same scene at [samples, lines] = [591-676, 158-240] and includes six classes.




![image-20241219182507218](./assets/image-20241219182507218.png)




Groundtruth classes for the Salinas-A scene and their respective samples number

|  #   |           Class           | Samples |
| :--: | :-----------------------: | :-----: |
|  1   |   Brocoli_green_weeds_1   |   391   |
|  2   | Corn_senesced_green_weeds |  1343   |
|  3   |    Lettuce_romaine_4wk    |   616   |
|  4   |    Lettuce_romaine_5wk    |  1525   |
|  5   |    Lettuce_romaine_6wk    |   674   |
|  6   |    Lettuce_romaine_7wk    |   799   |






[^Salinas-A]: https://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Salinas-A_scene



#### 7.1.1.13 XiongAn

[Link](http://www.hrs-cas.com/a/share/shujuchanpin/2019/0501/1049.html)[^XiongAn].

XiongAn: The XiongAn dataset was captured over the XiongAn field on November 1, 2018, through a visible shortwaveinfrared (SWIR) advanced hyperspectral imager (AHSI). The AHSI sensor is designed and produced by the Shanghai Institute of Techincal Physics, Chinese Academy of Sciences (CAS), and is mounted on China’s Gaofen5 satellite platform. Its height and width are 1400, respectively, creating 1 960 000 pixels in total with 77 937 labeled pixels in 20 classes. The spatial resolution of the HSI is 30 m/pixel, and the AHSI sensor collects 330 bands including 150 visible and nearinfrared (VNIR) bands and 180 SWIR bands for each pixel. The span of the bands ranges from 0.4 to 2.5 μm. However, there are 280 reserved bands after discarding 50 noisy bands. Compared with the other datasets, the XiongAn dataset has more samples (both labeled and unlabeled) and more categories. Meanwhile, the low spatial resolution easily generates mixed pixels. The ground object distribution of this HSI is more concentrated than the other HSIs. 




![image-20241219182535875](./assets/image-20241219182535875.png)

|  #   |          Class          | Samples |
| :--: | :---------------------: | :-----: |
|  1   |          Water          | 225647  |
|  2   |          Rice           | 180766  |
|  3   |      Acer_Compound      |  15353  |
|  4   |       Grass land        | 452144  |
|  5   |          Wilow          | 475591  |
|  6   |     Japonica sophra     | 169342  |
|  7   |        White wax        |  23304  |
|  8   |          Corn           | 165647  |
|  9   |          Peach          |  38409  |
|  10  |        Pear tree        | 193830  |
|  11  |         Poplar          |  5612   |
|  12  |         Soybean         |  59165  |
|  13  |    Vegetable fields     | 1026513 |
|  14  |           Elm           |  7151   |
|  15  |        Bare land        |  91072  |
|  16  | Koelreuteria paniculata |  29148  |
|  17  |      Rice stubble       |  1496   |
|  18  |         locust          | 421790  |
|  19  |      Spatse forest      |  65514  |
|  20  |         Houses          |  29616  |






[^XiongAn]: https://dx.doi.org/10.11834/jrs.20209065



#### 7.1.1.14 Xuzhou

[Link](http://www.hrs-cas.com/a/share/shujuchanpin/201) [^Xuzhou].

 The dataset is based on the study area of Xuzhou City, Jiangsu Province, China, and was captured by the HYSPEX hyperspectral camera. The dataset contains 436 bands with an image spatial size of 500 by 260 pixels.


![image-20241219182549829](./assets/image-20241219182549829.png)

|  #   |   Class    | Samples |
| :--: | :--------: | :-----: |
|  1   | Bareland_1 |  26396  |
|  2   |   Lakes    |  4027   |
|  3   |   Coals    |  2783   |
|  4   |   Cement   |  5214   |
|  5   |  Crops_1   |  13184  |
|  6   |   Trees    |  2436   |
|  7   | Bareland_2 |  6990   |
|  8   |  Crops_2   |  4777   |
|  9   | Red-tiles  |  3070   |






[^XuZhou]: http://www.hrs-cas.com/a/share/shujuchanpin/201



#### 7.1.1.15 HyMars

We proposed three datasets for Mars exploration, including scenes from Holden Crater (HC), Nili Fossae (NF), and Utopia Planitia (UP). The data was obtained from the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) aboard the Mars Reconnaissance Orbiter (MRO). Specifically, the HC is notable for its fluviolacustrine geological evolution, although the specifics of this evolution, such as water level changes and timing, remain unclear. The HC dataset consists of 418×595 pixels and retains 440 bands after filtering out water absorption and noise spectra. NF is unique for containing additional mineral phases like carbonates and serpentine, which suggests possible hydrothermal activity—an important factor for the potential emergence of biological activity on Mars. The NF dataset has dimensions of 478×593 pixels and includes 425 bands after discarding several noisy channel. UP, the landing site for the Tianwen-1 rover 'Zhurong', has a dataset size of 478×595×432.[^HyMars]

[^HyMars]:https://doi.org/10.57760/sciencedb.19732

​	HyMars系列数据集: 由Xi等人 (2025) 针对火星高光谱图像分类任务标注和发布的数据集，包括来自Holden Crater (HC), Nili Fossae (NF), 和 Utopia Planitia (UP) 等火星区域的CRISM数据。这些数据集的出现，将推动HSI分类技术在深空探测和行星科学领域的应用[^13]。



### 7.1.2 常用评估指标:

#### 7.1.2.1 Overall Accuracy (OA)：

​	总体精度 (Overall Accuracy, OA): 定义为被正确分类的测试像元总数与测试像元总数的比率。OA是衡量分类器整体性能最常用的指标，但当数据集中类别样本数量不均衡时，OA可能会被数量占优的类别所主导，不能完全反映模型对稀有类别的分类能力[^2]。

​	整体分类精度，即所有正确分类像素占总像素的比例。OA直观反映分类器的总体性能，但对类别不平衡数据不够敏感。
$$
OA = \frac{{\sum_{i=1}^k}{n_{ii}}}{N}
$$

* $n_{ii}$:矩阵中第 i 行第 i 列的值（表示正确分类的像素数）
* $k$:类别总数
* $N$:总像素数（混淆矩阵所有元素的和）



#### 7.1.2.2 Average Accuracy (AA)：

​	平均精度 (Average Accuracy, AA): 定义为所有单个类别的分类精度（即每个类别被正确分类的像元数与该类别总像元数的比率）的算术平均值。AA能够更好地反映模型在各个类别上的平均表现，尤其是在类别不均衡的情况下，AA比OA更能体现模型对少数类别的识别能力[^3]。

​	对每个类别的分类精度取平均值，更能体现分类器对小类别的性能。
$$
AA = \frac{1}{k}{\sum_{i=1}^k}{\frac{n_{ii}}{\sum_{j=1}^{k}{n_{ij}}}}
$$

* $n_{ii}$:矩阵中第 i 类正确分类的像素数
* $\sum_{j=1}^{k}{n_{ij}}$:混淆矩阵中第 i 类的总像素数（真实的第 i 类像素数）
* $k$:类别总数



#### 7.1.2.3 Kappa系数：

​	Kappa系数 (Kappa Coefficient, κ): Kappa系数是一种衡量分类结果与地面真实数据之间一致性的统计指标，它不仅考虑了正确分类的比例，还剔除了由随机机会导致的一致性。Kappa值通常在-1到1之间，值越接近1表示一致性越好，分类性能越优。Kappa系数对类别不均衡问题不敏感，被认为是比OA更稳健的评估指标[^2]。

衡量分类结果与随机分类结果之间的一致性，相比OA更能反映分类器的鲁棒性，特别是在类别不平衡情况下。
$$
kappa=\frac{OA-P_e}{1-P_e}
$$

* OA:Overall Accuracy
* $P_e$:随机分类的预期准确率

$$
P_e = \frac{\sum_{i=1}^{k}{({\sum_{j=1}^{k}{n_{ij}}}\cdot{\sum_{j=1}^{k}{n_{ji}}})}}{N^2}
$$

* $\sum_{j=1}^{k} n_{ij}$ ：混淆矩阵中第 i 类的真实像素总数（行和）
*  $\sum_{j=1}^{k} n_{ji}$ ：混淆矩阵中第 i 类的预测像素总数（列和）
*  $N$ ：总像素数

解释：

$kappa \in [-1,1]$

* $kappa = 1$:表示分类完全正确
* $kappa = 0$:表示分类结果与随机分类一致
* $kappa < 0$:表示分类结果比随机分类更差



​	其他指标:

​	F1分数 (F1-Score): 精确率（Precision）和召回率（Recall）的调和平均数，综合评价模型的查准和查全能力，常用于评估单个类别的性能或计算宏平均/微平均F1分数[^18]。

​	精确率 (Precision): 对于某个类别，被正确预测为该类别的像元数占所有被预测为该类别的像元数的比例。

​	召回率 (Recall) / 生产者精度 (Producer's Accuracy): 对于某个类别，被正确预测为该类别的像元数占该类别真实像元总数的比例。

​	用户精度 (User's Accuracy): 与精确率含义相同。

​	混淆矩阵 (Confusion Matrix): 一个N×N的矩阵（N为类别数），详细记录了每个类别的真实标签与预测标签之间的对应关系，是计算上述多数指标的基础。

​	计算时间 (Computational Time) / 参数量 (Number of Parameters): 用于评估模型的效率和复杂度，对于实际应用和部署至关重要 [^9]。



不同指标的优劣分析：

OA的局限性：对于类别不平衡的数据集，OA可能被大类的高精度掩盖，而忽视小类的低精度。

AA的优势：通过均匀考虑各类别，AA能够更好地评估分类算法对多类数据的综合性能，但不考虑样本数量差异。

Kappa系数的可靠性：作为一种基于随机一致性的评估指标，Kappa系数在类别不平衡场景下表现更稳定，但计算复杂性略高于OA和AA。

​	

 

# 8. 各类方法的综合比较与讨论

为了系统性地理解前述各类深度学习方法在高光谱图像分类任务中的特性，本章将从特征提取能力、对核心挑战的应对策略、计算效率与模型复杂度以及发展趋势等多个维度，对它们进行横向的比较与深入的讨论。



## **8.1 特征提取能力的对比：从局部到全局，从欧氏到非欧氏**

- **CNN：** 核心优势在于强大的**局部特征提取能力**。通过卷积核的局部连接和权值共享，CNN能高效地学习具有平移不变性的空谱局部模式，如纹理、边缘和光谱吸收谷的局部形状。然而，其感受野受限于卷积核大小和网络深度，对**全局长距离依赖关系**的建模能力天然不足。
- **RNN：** 专注于**序列依赖关系**的建模。它将光谱视为一维序列，擅长捕捉相邻波段间的相关性。以LSTM/GRU为代表的门控RNN虽能学习长程依赖，但其循环计算的特性使其在处理二维空间信息时存在天然障碍，且并行性较差。
- **GNN：** 开创了在**非欧氏空间**中进行特征学习的范式。通过将像元或超像元建模为图节点，GNN能够灵活地捕捉不规则的地物边界和复杂的空间拓扑关系，其邻域聚合机制为学习**高阶关系和全局上下文**提供了有力工具。
- **Transformer：** 其核心优势在于凭借自注意力机制实现的**全局感受野**。它能够直接建模输入序列中任意两个元素间的关系，从而高效地捕捉长距离依赖，非常适合学习高阶语义特征和全局空间上下文。但其对局部细节的感知能力不如CNN。
- **Mamba：** 作为一种新型序列模型，Mamba同样具备强大的**长程依赖建模能力**，且在理论上通过选择性状态空间机制实现了对信息的有效压缩和传递。它为全局上下文建模提供了一个计算效率更高的新选择。

## **8.2 对核心挑战的应对策略**

| 核心挑战            | CNN策略                                                      | GNN策略                                                      | Transformer/Mamba策略                                        |
| ------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **高维性/休斯现象** | 采用1x1卷积、PCA降维、轻量化设计（如深度可分离卷积）来降低特征维度和模型参数量。 | 基于超像元构建图，显著减少节点数量；Mini-batch训练策略（如MiniGCN）处理大规模图。 | 先通过CNN进行特征提取和降维，缩短输入序列长度；采用线性复杂度的注意力变体或Mamba模型。 |
| **有限标记样本**    | 数据增强；迁移学习；设计参数更少的紧凑模型（如Fast Compact 3D-CNN）；胶囊网络（DC-CapsNet）。 | 天然的半监督学习框架，可同时利用标记和未标记样本；图对比学习（如MCTGCL）增强特征可分性。 | 采用CNN-Transformer混合架构，利用CNN较强的归纳偏置降低对数据量的依赖；自监督预训练。 |
| **光谱变异性**      | 引入注意力机制，自适应地关注判别性强的波段；多尺度特征融合，捕捉不同层次的光谱模式。 | 动态图构建，根据特征相似性自适应调整邻居关系；图注意力网络（GAT）为不同邻居分配不同权重。 | 自注意力/选择性机制能够根据输入内容动态调整权重，对输入变化具有更强的适应性。 |



## **8.3 计算效率与模型复杂度**

- **3D-CNN** 在实现空谱联合提取的同时，也带来了巨大的参数量和计算开销，是计算密集型模型的代表。
- **GNN** 的复杂度与图中节点数和边数相关，对于基于像元构建的大规模图，计算成本依然高昂。
- **Transformer** 的标准自注意力机制具有关于序列长度的**二次复杂度 (O(N2))**，是其应用于高分辨率或高维数据时的主要瓶颈。
- **Mamba** 以其**线性复杂度 (O(N))** 脱颖而出，为高效处理超长序列提供了极具吸引力的解决方案，在计算效率上优势显著。

## **8.4 总结与趋势**

综上所述，没有任何一种单一模型是完美的。该领域的发展清晰地呈现出从“单一模型创新”向“**多模型优势互补的混合架构**”演进的趋势。CNN强大的局部特征提取能力，GNN灵活的关系建模能力，以及Transformer/Mamba卓越的全局和长程依赖捕捉能力，共同构成了现代高光谱图像分类模型的“能力版图”。未来的研究将更侧重于如何将这些基础模块进行更高效、更智能的组合与协同，以在精度、效率和鲁棒性之间达到新的平衡。



# 9. 总结与展望

​	高光谱图像分类作为遥感领域的核心研究课题之一，在过去的五年中，随着深度学习和机器学习技术的飞速发展，取得了显著的进展。本综述系统地回顾了基于卷积神经网络（CNN）、循环神经网络（RNN）、图神经网络（GNN）、Transformer以及新兴的Mamba（状态空间模型）等主流方法在高光谱图像分类任务中的最新研究成果，并对它们的原理、创新点、性能表现及优缺点进行了分析和讨论。

## 总结:

​	CNN的基石作用与持续演进: CNN，特别是3D-CNN及其混合变体（如HybridSN, Fast & Compact 3D-CNN, DC-CapsNet），通过有效提取空谱联合特征，为HSI分类奠定了坚实基础。研究趋势从最初的1D/2D-CNN发展到3D-CNN，再到追求更高效率和更强表达能力的轻量化、多尺度、注意力增强及混合CNN架构。CNN在局部特征感知方面的优势使其仍是许多先进模型不可或缺的组成部分。

​	RNN的序列建模潜力: RNN及其门控变体（LSTM, GRU）将像元光谱视为序列进行处理，为捕捉光谱间的顺序依赖和动态变化提供了独特途径。尽管纯RNN模型因空间信息处理不足和计算效率问题应用受限，但其作为特征提取模块嵌入到CNN-RNN等混合架构中，或在GRNN等图模型中发挥作用，仍显示出一定潜力。

​	GNN的非欧氏数据处理优势: GNN（尤其是GCN及其改进型如MiniGCN, Nonlocal GCN, MSDesGATnet）通过将HSI数据建模为图结构，能够灵活处理非欧氏空间信息，有效捕捉像元间的复杂关系和不规则地物边界。多尺度图、动态图以及注意力机制的引入进一步增强了GNN的表达能力和适应性。CNN-GNN混合模型已成为一个重要的研究方向。

​	Transformer的全局上下文建模突破: Transformer凭借其自注意力机制带来的全局感受野，在捕捉HSI中的长距离依赖和高级语义特征方面表现出色（如SSFTT, MCTGCL, SpectralFormer）。CNN-Transformer混合模型通过结合CNN的局部特征提取能力和Transformer的全局建模优势，已成为当前性能最优异的方法之一。然而，Transformer的二次计算复杂度仍是其应用于大规模HSI数据时需要重点考虑的问题。

​	Mamba的线性复杂度新机遇: Mamba作为一种基于状态空间模型的新型序列建模架构，以其线性的计算复杂度和强大的长程依赖建模能力，为解决Transformer的效率瓶颈提供了有希望的替代方案。尽管其在HSI分类中的应用尚处于早期探索阶段（如HS-Mamba, MambaLG, GraphMamba），但已展现出巨大潜力，特别是在与CNN、GNN等模型结合方面。

​	混合模型的趋势: 单一模型往往难以完美应对HSI数据的多重挑战（高维性、小样本、空谱复杂性等）。因此，设计能够融合不同模型优势的混合架构（如CNN-Transformer, CNN-GNN, Mamba-GNN, CNN-RNN）已成为提升HSI分类性能的主流趋势。这种“分工协作”的设计哲学，使得模型能够更全面地从HSI数据中提取和利用信息。

​	数据与效率的重要性日益凸显: 随着模型精度的不断提升，“小样本学习”的挑战依然严峻。同时，模型的计算效率、参数量以及在资源受限平台上的可部署性，已成为评价模型实用性的关键指标。新的、更具挑战性的基准数据集的出现，也在推动模型向更强的泛化能力和对真实复杂场景的适应性发展。

​	展望未来，高光谱图像分类领域的研究可能将聚焦于以下几个方向：

​	更高效、更轻量化的模型架构: 持续探索具有更低计算复杂度（如Mamba的线性复杂度思想）、更少参数量但保持高性能的模型。这包括对现有CNN、Transformer、Mamba等基础模块的优化，以及更精巧的混合模型设计和模型压缩技术（如剪枝、量化、知识蒸馏）的应用。

​	深度融合物理先验与数据驱动: 将遥感图像的光谱物理模型、成像机理等先验知识更深度地融入到深度学习模型的设计中，例如通过物理约束的损失函数、可解释的模块设计等，有望提升模型在小样本、噪声干扰等情况下的鲁棒性和泛化能力，并增强模型的可解释性。

​	自监督、半监督与无监督学习的突破: 鉴于标记样本的稀缺性，充分利用大量无标记高光谱数据进行自监督或半监督预训练，学习通用的、可迁移的特征表示，将是提升模型性能和泛化能力的关键。无监督或弱监督分类方法的研究也将持续受到关注。

​	多模态数据融合与跨域学习: 结合高光谱与其他遥感数据（如LiDAR、SAR、高空间分辨率多光谱图像）进行多模态融合分类，能够提供更丰富、更互补的信息，有望进一步提升分类精度和对复杂场景的理解能力。同时，研究如何将在一个数据集或传感器上学到的知识迁移到另一个数据集或传感器（跨域学习），对于提升模型的实用性至关重要。

​	面向特定应用场景的定制化模型: 针对精准农业、环境监测、城市规划、行星探测等具体应用需求，设计和优化专门的HSI分类模型。例如，针对火星高光谱数据分类的MCTGCL模型就是一个很好的例子。这需要更紧密地结合领域知识和应用目标。

​	模型可解释性与不确定性量化: 深度学习模型常被诟病为“黑箱”。增强模型的可解释性，理解其决策过程，对于建立用户信任和指导模型改进至关重要。同时，对分类结果进行不确定性量化，评估预测的可靠性，对于许多实际应用（如风险评估、决策支持）具有重要价值（如[^41]中对GCN不确定性的研究）。

​	图学习与因果推断的深入探索: GNN在关系建模方面展现了优势，未来可以进一步探索更强大的图学习方法，如动态图学习、异构图学习、超图学习等。同时，尝试将因果推断的思想引入HSI分类，从“相关性”走向“因果性”分析，可能为理解地物光谱形成的复杂机制和提升分类的根本鲁棒性开辟新途径。

​	总之，高光谱图像分类是一个充满活力和挑战的研究领域。随着深度学习理论的不断发展、计算能力的持续增强以及高质量数据集的日益丰富，我们有理由相信，未来将会涌现出更多创新性的、高效且鲁棒的分类方法，为高光谱遥感技术的广泛应用提供更强大的支撑。

## **9.2 展望**

展望未来，高光谱图像分类领域的研究将在深度、广度和效率等多个维度上持续演进，可能将聚焦于以下几个前沿方向：

- **1. 更高效、更轻量化的模型架构：** 面对日益增长的数据规模和星上、机载等资源受限平台的应用需求，持续探索具有更低计算复杂度（如推广Mamba的线性复杂度思想）、更少参数量但保持高性能的模型将是核心议题。这包括对现有基础模块的优化，以及更精巧的混合模型设计和模型压缩技术（如网络剪枝、知识蒸馏）的深度应用。
- **2. 物理先验与数据驱动的深度融合：** 将遥感图像的光谱物理模型、成像机理等先验知识更深度地融入深度学习模型。例如，通过设计物理约束的损失函数或可解释的物理模型模块，有望显著提升模型在小样本、强噪声等恶劣条件下的鲁棒性和泛化能力，并为“黑箱”模型提供物理解释。
- **3. 自监督与半监督学习的范式突破：** 鉴于标记样本的稀缺性是长期存在的瓶颈，充分利用海量无标记高光谱数据进行自监督或半监督预训练，学习通用的、可迁移的特征表示，是提升模型性能和泛化能力的关键路径。基于对比学习、掩码建模等先进范式的研究将持续升温。
- **4. 多模态数据融合与跨域学习：** 单一高光谱数据源的信息有限。结合LiDAR、SAR、高空间分辨率图像等多模态数据进行融合分类，能够提供更丰富、更互补的信息，是攻克复杂场景分类难题的有效途径。同时，研究跨数据集、跨传感器的领域自适应（Domain Adaptation）技术，对于提升模型的现实应用价值至关重要。
- **5. 模型可解释性与不确定性量化：** 增强模型决策过程的可解释性（XAI），对于建立用户信任、指导模型改进至关重要。同时，对分类结果进行不确定性量化与评估（如[41]），为决策者提供预测的可靠性信息，在风险评估、精准农业等关键应用中具有不可或缺的价值。
- **6. 前沿图学习与因果推断的探索：** GNN已展现出强大的关系建模能力。未来可进一步探索动态图学习、异构图学习、超图等更强大的图学习方法。同时，尝试将因果推断的思想引入HSI分类，探索从“相关性”分析走向“因果性”推理，可能为理解地物光谱形成的复杂机制和提升分类的根本鲁棒性开辟全新途径。



# 引用

[^1]: FLOP-Reduction Through Memory Allocations Within CNN for Hyperspectral Image Classification - UMBC, https://www2.umbc.edu/rssipl/people/aplaza/Papers/Journals/2021.TGRS.Memory.pdf
[^2]: HS-Mamba: Full-Field Interaction Multi-Groups Mamba for ... - arXiv, http://arxiv.org/pdf/2504.15612
[^3]: Tri-CNN: A Three Branch Model for Hyperspectral Image Classification - MDPI, https://www.mdpi.com/2072-4292/15/2/316
[^4]: Machine Learning and Deep Learning Techniques for Spectral Spatial Classification of Hyperspectral Images: A Comprehensive Survey - MDPI, https://www.mdpi.com/2079-9292/12/3/488
[^5]: Full article: Bridging branches and attributes: spectral-spatial global-local interaction network for hyperspectral image classification - Taylor & Francis Online, https://www.tandfonline.com/doi/full/10.1080/01431161.2025.2457130?af=R
[^6]: Adaptive Multi-Feature Fusion Graph Convolutional Network for Hyperspectral Image Classification - MDPI, https://www.mdpi.com/2072-4292/15/23/5483
[^7]: HyperKAN: Kolmogorov-Arnold Networks make Hyperspec- tral Image Classifiers Smarter - arXiv, https://arxiv.org/pdf/2407.05278
[^8]: A review on the combination of deep learning techniques with proximal hyperspectral images in agriculture - ScienceDirect | PDF - Scribd, https://www.scribd.com/document/817135740/A-review-on-the-combination-of-deep-learning-techniques-with-proximal-hyperspectral-images-in-agriculture-ScienceDirect
[^9]: arxiv.org, https://arxiv.org/pdf/2402.10026
[^10]: CMTNet: a hybrid CNN-transformer network for UAV-based hyperspectral crop classification in precision agriculture - PubMed, https://pubmed.ncbi.nlm.nih.gov/40216979/
[^11]: SSATNet: Spectral-spatial attention transformer for hyperspectral corn image classification - Frontiers, https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2024.1458978/full
[^12]: (PDF) Hybrid State-Space and GRU-based Graph Tokenization ..., https://www.researchgate.net/publication/388884045_Hybrid_State-Space_and_GRU-based_Graph_Tokenization_Mamba_for_Hyperspectral_Image_Classification
[^13]: MCTGCL: Mixed CNN-Transformer for Mars Hyperspectral Image ..., https://www.researchgate.net/publication/388050716_MCTGCL_Mixed_CNN-Transformer_for_Mars_Hyperspectral_Image_Classification_With_Graph_Contrastive_Learning
[^14]: A deep learning model using hyperspectral image for EUS‐FNA cytology diagnosis in pancreatic ductal adenocarcinoma - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC10501295/
[^15]: Deep Learning in Medical Hyperspectral Images: A Review - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC9784550/
[^16]: (PDF) A review of hyperspectral image classification based on graph ..., https://www.researchgate.net/publication/389908802_A_review_of_hyperspectral_image_classification_based_on_graph_neural_networks
[^17]: Full article: Multi-Scale Dense Graph Attention Network for ..., https://www.tandfonline.com/doi/full/10.1080/07038992.2024.2333424
[^18]: A Fast and Compact 3-D CNN for Hyperspectral Image Classification - ResearchGate, https://www.researchgate.net/publication/347957604_A_Fast_and_Compact_3-D_CNN_for_Hyperspectral_Image_Classification
[^19]: Robust hyperspectral image classification using generative adversarial networks - DOI, https://doi.org/10.1016/j.ins.2024.120452
[^20]: Improving Hyperspectral Image Classification with Compact Multi-Branch Deep Learning, https://www.mdpi.com/2072-4292/16/12/2069
[^21]: Full article: Spectral Spatial Neighborhood Attention Transformer for Hyperspectral Image Classification - Taylor & Francis Online: Peer-reviewed Journals, https://www.tandfonline.com/doi/full/10.1080/07038992.2024.2347631
[^22]: Segmentation-based deep 2D-3D multibranch learning approach for effective hyperspectral image classification - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC12124512/
[^23]: apps.dtic.mil, https://apps.dtic.mil/sti/trecms/pdf/AD1124677.pdf
[^24]: Cascaded Recurrent Neural Networks for Hyperspectral Image Classification | Request PDF - ResearchGate, https://www.researchgate.net/publication/331600148_Cascaded_Recurrent_Neural_Networks_for_Hyperspectral_Image_Classification
[^25]: HYPERSPECTRAL IMAGE CLASSIFICATION USING RESIDUAL 2D AND 3D CONVOLUTIONAL NEURAL NETWORK JOINT ATTENTION MODEL - ISPRS - The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, https://isprs-archives.copernicus.org/articles/XLIV-M-3-2021/187/2021/
[^26]: Deep Recurrent Neural Networks for Hyperspectral Image Classification - DLR, https://elib.dlr.de/114208/1/07914752.pdf
[^27]: (PDF) Assessing CNN and Semantic Segmentation Models for ..., https://www.researchgate.net/publication/384424708_Assessing_CNN_and_Semantic_Segmentation_Models_for_Coarse_Resolution_Satellite_Image_Classification_in_Subcontinental_Scale_Land_Cover_Mapping
[^28]: (PDF) Comparative Analysis and Implication of Hyperion ..., https://www.researchgate.net/publication/374213562_Comparative_Analysis_and_Implication_of_Hyperion_Hyperspectral_and_Landsat-8_Multispectral_Dataset_in_Land_Classification
[^29]: CMC | Free Full-Text | Improving Generalization for Hyperspectral Image Classification: The Impact of Disjoint Sampling on Deep Models, https://www.techscience.com/cmc/v81n1/58367/html
[^30]: Nearshore seabed topography reconstruction method based on convolutional neural network - ScienceDirect - DOI, https://doi.org/10.1016/j.eswa.2025.127982
[^31]: (PDF) Hyperspectral Remote Sensing Image Classification Using ..., https://www.researchgate.net/publication/354308861_Hyperspectral_Remote_Sensing_Image_Classification_Using_Deep_Convolutional_Capsule_Network
[^32]: Network Collaborative Pruning Method for Hyperspectral Image Classification Based on Evolutionary Multi-Task Optimization - MDPI, https://www.mdpi.com/2072-4292/15/12/3084
[^33]: ieeexplore.ieee.org, https://ieeexplore.ieee.org/document/9303477
[^34]: Hyperspectral image classification using spectral-spatial LSTMs - Web Pages, http://h.web.umkc.edu/hangr/papers/Hyperspectral image classification using spectral-spatial LSTMs.pdf
[^35]: Graph neural networks for multi-sensor Earth observation - ScienceDirect - DOI, https://doi.org/10.1016/b978-0-44-326484-9.00020-8
[^36]: Full article: Classification of hyperspectral images using fusion of CNN and MiniGCN with SVM, https://www.tandfonline.com/doi/full/10.1080/17538947.2023.2229793
[^37]: PGNN-Net: Parallel Graph Neural Networks for Hyperspectral Image Classification Using Multiple Spatial-Spectral Features - MDPI, https://www.mdpi.com/2072-4292/16/18/3531
[^38]: www2.umbc.edu, https://www2.umbc.edu/rssipl/people/aplaza/Papers/Journals/2021.TGRS.Graph.pdf
[^39]: How do graph-based neural networks compare to traditional CNNs for hyperspectral image classification? - Consensus, https://consensus.app/search/how-do-graph-based-neural-networks-compare-to-trad/caDryURvSZum5sLolfWIJQ/
[^40]: Exploring multi-relational spatial interaction imputation with distance-decay effects, https://www.tandfonline.com/doi/full/10.1080/17538947.2023.2300316
[^41]: Uncertainty-aware Graph-based Hyperspectral Image Classification - OpenReview, https://openreview.net/forum?id=8dN7gApKm3
[^42]: Smart Mining System with Crystal Classification of Ores and Industrial Management - EasyChair, https://easychair.org/publications/paper/qTx2/open
[^43]: ieeexplore.ieee.org, https://ieeexplore.ieee.org/document/9093000
[^44]: ieeexplore.ieee.org, https://ieeexplore.ieee.org/document/9242719
[^45]: ieeexplore.ieee.org, https://ieeexplore.ieee.org/document/8825100
[^46]: , https://ieeexplore.ieee.org/document/9151169
[^47]: , https://www.researchgate.net/publication/343661058_Graph_Convolutional_Networks_for_Hyperspectral_Image_Classification
[^48]: How do graph convolutional networks improve hyperspectral image classification?, https://consensus.app/search/how-do-graph-convolutional-networks-improve-hypers/0wxrbyFNRHGLnCFYd2qBgg/
[^49]: 1 - OUCI, https://ouci.dntb.gov.ua/en/?publisher=IEEE&exclude=2644-1276,1558-2183&access=open&sort=crossref-backlinks&edition_intdb=wos&journal=IEEE+Transactions+on+Geoscience+and+Remote+Sensing&year_from=2020&p=1
[^50]: Hyperspectral Image Classification With Localized Spectral Filtering-Based Graph Attention Network - International Society for Photogrammetry and Remote Sensing, https://www.isprs.org/medialib/presentation.aspx?ID=1130&Path=/medialib/nice2022/
[^51]: Track: Deep Learning Applications - ICML 2025, https://icml.cc/virtual/2021/session/11967
[^52]: Top 4003 IEEE Transactions on Geoscience and Remote Sensing papers published in 2022 - SciSpace, https://scispace.com/journals/ieee-transactions-on-geoscience-and-remote-sensing-28l5dw3p/2022
[^53]: arxiv.org, https://arxiv.org/html/2406.14080v2
[^54]: A U-Shaped Convolution-Aided Transformer with Double Attention for Hyperspectral Image Classification - ResearchGate, https://www.researchgate.net/publication/377340239_A_U-Shaped_Convolution-Aided_Transformer_with_Double_Attention_for_Hyperspectral_Image_Classification/fulltext/65a1cfa2af617b0d87418d0c/A-U-Shaped-Convolution-Aided-Transformer-with-Double-Attention-for-Hyperspectral-Image-Classification.pdf
[^55]: Full article: A hybrid convolution transformer for hyperspectral image classification - Taylor & Francis Online: Peer-reviewed Journals, https://www.tandfonline.com/doi/full/10.1080/22797254.2024.2330979
[^56]: A Spatial–Spectral Transformer for Hyperspectral Image Classification Based on Global Dependencies of Multi-Scale Features - MDPI, https://www.mdpi.com/2072-4292/16/2/404
[^57]: SSATNet: Spectral-spatial attention transformer for hyperspectral corn image classification - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC11781253/
[^58]: Spectral-Spatial Feature Tokenization Transformer for Hyperspectral ..., https://www.researchgate.net/publication/357918405_Spectral-Spatial_Feature_Tokenization_Transformer_for_Hyperspectral_Image_Classification
[^59]: DCT-Mamba3D: Spectral Decorrelation and Spatial-Spectral Feature Extraction for Hyperspectral Image Classification - arXiv, https://arxiv.org/html/2502.01986v1
[^60]: Interactive Spectral-Spatial Transformer for Hyperspectral Image Classification | Request PDF - ResearchGate, https://www.researchgate.net/publication/379741425_Interactive_Spectral-Spatial_Transformer_for_Hyperspectral_Image_Classification
[^61]: ieeexplore.ieee.org, https://ieeexplore.ieee.org/document/9784034
[^62]: , https://www.researchgate.net/publication/360718045_Spectral-Spatial_Feature_Tokenization_Transformer_for_Hyperspectral_Image_Classification
[^63]: CMTNet: a hybrid CNN-transformer network for UAV-based hyperspectral crop classification in precision agriculture - ResearchGate, https://www.researchgate.net/publication/390697098_CMTNet_a_hybrid_CNN-transformer_network_for_UAV-based_hyperspectral_crop_classification_in_precision_agriculture
[^64]: , https://arxiv.org/pdf/2406.14080.pdf
[^65]: B-Xi/TGRS_2025_MCTGCL: MCTGCL: Mixed CNN-Transformer for Mars Hyperspectral Image Classification With Graph Contrastive Learning, TGRS, 2025 - GitHub, https://github.com/B-Xi/TGRS_2025_MCTGCL
[^66]: HyMars: Mars Hyperspectral Image Classification Benchmark Datasets, https://www.scidb.cn/en/detail?dataSetId=4ff0774d45464f239a73f37796f7a786
[^67]: Tie Zheng's research works | Chinese Academy of Sciences and other places, https://www.researchgate.net/scientific-contributions/Tie-Zheng-2220724286
[^68]: ieeexplore.ieee.org, https://ieeexplore.ieee.org/document/10503187
[^69]: Hyperspectral Image Classification With Mamba | Request PDF, https://www.researchgate.net/publication/387360398_Hyperspectral_Image_Classification_with_Mamba
[^70]: GASSM - OpenAIR@RGU, https://rgu-repository.worktribe.com/file/2663004/1/LI 2025 GASSM (VOR)
